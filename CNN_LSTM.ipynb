{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT THE LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOISE\n",
    "def noise(data):\n",
    "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "# STRETCH\n",
    "def stretch(data, rate):\n",
    "    return librosa.effects.time_stretch(y=data,rate=rate)\n",
    "# SHIFT\n",
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
    "    return np.roll(data, shift_range)\n",
    "# PITCH\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(y=data, sr=sampling_rate, n_steps=pitch_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(X,sr,mfcc=True,zcr=False,rmse=False,spectral_rolloff=False,spectral_contrast=False,spectral_bandwidth=False,spectral_centroid=False,\n",
    "                    signal_mean=False,signal_std=False,signal_duration=False,temporal_centroid=False,chroma=False, mel=False, signal_skewness=False,signal_kurtosis=False):\n",
    "    # X, sample_rate = librosa.load(file_path,duration=2.5,offset=0.6)\n",
    "    result=np.array([])\n",
    "    if mfcc:\n",
    "        mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40).T, axis=0)\n",
    "        result=np.hstack((result, mfccs))\n",
    "        \n",
    "    if zcr:\n",
    "        zcr=np.squeeze(librosa.feature.zero_crossing_rate(X))\n",
    "        result=np.hstack((result, zcr))\n",
    "        \n",
    "    if rmse:    \n",
    "        rmse = np.mean(librosa.feature.rms(y=X), axis=1)\n",
    "        result=np.hstack((result, rmse))\n",
    "        \n",
    "    if spectral_rolloff:\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=X, sr=sr), axis=1) \n",
    "        result=np.hstack((result, spectral_rolloff))  \n",
    "    \n",
    "    if spectral_contrast:\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=X, sr=sr).T, axis=0) \n",
    "        result=np.hstack((result, spectral_contrast)) \n",
    "        \n",
    "    if spectral_bandwidth:\n",
    "        spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=X, sr=sr), axis=1) \n",
    "        result=np.hstack((result, spectral_bandwidth)) \n",
    "        \n",
    "    if spectral_centroid:\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=X, sr=sr), axis=1) \n",
    "        result=np.hstack((result, spectral_centroid)) \n",
    "        \n",
    "    if signal_mean:\n",
    "        signal_mean = np.mean(X)\n",
    "        result=np.hstack((result, signal_mean))\n",
    "        \n",
    "    if signal_std:\n",
    "        signal_std = np.std(X)\n",
    "        result=np.hstack((result, signal_std))\n",
    "        \n",
    "    if signal_duration:\n",
    "        signal_duration = librosa.get_duration(y=X, sr=sr)\n",
    "        result=np.hstack((result, signal_duration))\n",
    "        \n",
    "    if temporal_centroid:\n",
    "        temporal_centroid = np.sum(np.arange(len(X)) * X) / np.sum(X)\n",
    "        result=np.hstack((result, temporal_centroid))\n",
    "         \n",
    "    if chroma:\n",
    "        stft=np.abs(librosa.stft(X))\n",
    "        chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T,axis=0)\n",
    "        result=np.hstack((result, chroma))\n",
    "        \n",
    "    if mel:\n",
    "        mel=np.mean(librosa.feature.melspectrogram(y=X, sr=sr).T,axis=0)\n",
    "        result=np.hstack((result, mel))\n",
    "        \n",
    "    if signal_skewness:\n",
    "        signal_mean = np.mean(X)\n",
    "        signal_std = np.std(X)\n",
    "        skewness = np.mean((X - signal_mean) ** 3) / signal_std ** 3\n",
    "        result=np.hstack((result, skewness))\n",
    "        \n",
    "    if signal_kurtosis:\n",
    "        signal_mean = np.mean(X)\n",
    "        signal_std = np.std(X)\n",
    "        kurtosis = np.mean((X - signal_mean) ** 4) / signal_std ** 4\n",
    "        result=np.hstack((result, kurtosis))\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(path,duration=2.5, offset=0.6):\n",
    "    data,sr=librosa.load(path,duration=duration,offset=offset)\n",
    "    aud=extract_feature(data,sr,mfcc=True,chroma=False, mel=False,zcr=False,rmse=False,spectral_rolloff=False,spectral_contrast=False,spectral_bandwidth=False,spectral_centroid=False,\n",
    "                    signal_mean=False,signal_std=False,signal_duration=False,temporal_centroid=False,\n",
    "                     signal_skewness=False,signal_kurtosis=False)\n",
    "    audio=np.array(aud)\n",
    "    \n",
    "    # noised_audio=noise(data)\n",
    "    # aud2=extract_feature(noised_audio,sr,mfcc=True,chroma=False, mel=False,zcr=False,rmse=True,spectral_rolloff=False,spectral_contrast=False,spectral_bandwidth=False,spectral_centroid=False,\n",
    "    #                 signal_mean=False,signal_std=False,signal_duration=False,temporal_centroid=False,\n",
    "    #                  signal_skewness=False,signal_kurtosis=False)\n",
    "    # audio=np.vstack((audio,aud2))\n",
    "    \n",
    "    # # stretch_audio=stretch(data=data, rate=0.8)\n",
    "    # # aud3=extract_feature(stretch_audio,sr,mfcc=True,zcr=True,rmse=True,spectral_rolloff=False,spectral_contrast=False,spectral_bandwidth=False,spectral_centroid=False,\n",
    "    # #                 signal_mean=False,signal_std=False,signal_duration=False,temporal_centroid=False,\n",
    "    # #                 chroma=True, mel=True, signal_skewness=False,signal_kurtosis=False)\n",
    "    # # audio=np.vstack((audio,aud3))\n",
    "    \n",
    "    # shift_audio=shift(data)\n",
    "    # aud4=extract_feature(shift_audio,sr,mfcc=True,chroma=False, mel=False,zcr=False,rmse=True,spectral_rolloff=False,spectral_contrast=False,spectral_bandwidth=False,spectral_centroid=False,\n",
    "    #                 signal_mean=False,signal_std=False,signal_duration=False,temporal_centroid=False,\n",
    "    #                  signal_skewness=False,signal_kurtosis=False)\n",
    "    # audio=np.vstack((audio,aud4))\n",
    "    \n",
    "    # pitched_audio=pitch(data,sr)\n",
    "    # aud5=extract_feature(pitched_audio,sr,mfcc=True,chroma=False, mel=False,zcr=False,rmse=True,spectral_rolloff=False,spectral_contrast=False,spectral_bandwidth=False,spectral_centroid=False,\n",
    "    #                 signal_mean=False,signal_std=False,signal_duration=False,temporal_centroid=False,\n",
    "    #                  signal_skewness=False,signal_kurtosis=False)\n",
    "    # audio=np.vstack((audio,aud5))\n",
    "    \n",
    "    # pitched_audio1=pitch(data,sr)\n",
    "    # pitched_noised_audio=noise(pitched_audio1)\n",
    "    # aud6=extract_feature(pitched_noised_audio,sr,mfcc=True,chroma=False, mel=False,zcr=False,rmse=True,spectral_rolloff=False,spectral_contrast=False,spectral_bandwidth=False,spectral_centroid=False,\n",
    "    #                 signal_mean=False,signal_std=False,signal_duration=False,temporal_centroid=False,\n",
    "    #                  signal_skewness=False,signal_kurtosis=False)\n",
    "    # audio=np.vstack((audio,aud6))\n",
    "    \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path=pd.read_csv(r\"C:\\Users\\youss\\Desktop\\Speech Recognition\\Code\\Github\\Ravdess_path.csv\")\n",
    "data_path=pd.read_csv(r\"C:\\Users\\youss\\Desktop\\Speech Recognition\\Code\\1\\RavTess.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4240it [00:52, 81.48it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "from tqdm import tqdm\n",
    "start = timeit.default_timer()\n",
    "X,Y=[],[]\n",
    "for path,emotion,index in tqdm (zip(data_path.Path,data_path.Emotions,range(data_path.Path.shape[0]))):\n",
    "    features=get_features(path)\n",
    "    # if index%500==0:\n",
    "    #     print(f'{index} audio has been processed')\n",
    "    # for i in features:\n",
    "    X.append(features)\n",
    "    Y.append(emotion)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emotions = pd.DataFrame(X)\n",
    "Emotions['Emotions'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4240, 41)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emotions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df= Emotions.dropna(axis=1)\n",
    "features=Df.drop('Emotions', axis=1)\n",
    "labels=Df[\"Emotions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and temporary sets (80% training, 20% temp)\n",
    "X_train_temp, X_test, y_train_temp, y_test = train_test_split(features,labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the temporary set into training and validation sets (75% training, 25% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.25, random_state=42)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Encode string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "y_val_encoded = label_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1596      angry\n",
      "2003       fear\n",
      "1955       fear\n",
      "1325    neutral\n",
      "2735        sad\n",
      "3400       fear\n",
      "848     neutral\n",
      "948     disgust\n",
      "2136      happy\n",
      "1097      happy\n",
      "Name: Emotions, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y_train[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense, Activation, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Base_Model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(64, 5, padding='same', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "\n",
    "    model.add(Conv1D(128, 5, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "\n",
    "    model.add(Conv1D(256, 5, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\youss\\miniconda3\\envs\\speech\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,096</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_18 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m384\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_33 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_17 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_19 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m41,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_34 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_18 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_20 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │       \u001b[38;5;34m164,096\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_35 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │         \u001b[38;5;34m4,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">209,672</span> (819.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m209,672\u001b[0m (819.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">209,672</span> (819.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m209,672\u001b[0m (819.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define input shape and number of classes\n",
    "input_shape = (X_train.shape[1], 1)  # Each audio frame has 40 features\n",
    "num_classes = 8  # Number of classes (Neutral, Calm, Happy, Sad, Angry, Fearful, Disgust, Surprised)\n",
    "\n",
    "# Create the model\n",
    "model = Base_Model(input_shape, num_classes)\n",
    "# opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=1e-07, decay=0.0)\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3614 - loss: 1.6136\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69104, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.3632 - loss: 1.6097 - val_accuracy: 0.6910 - val_loss: 0.9555\n",
      "Epoch 2/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7163 - loss: 0.8533\n",
      "Epoch 2: val_accuracy improved from 0.69104 to 0.74646, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7163 - loss: 0.8528 - val_accuracy: 0.7465 - val_loss: 0.7748\n",
      "Epoch 3/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7520 - loss: 0.7151\n",
      "Epoch 3: val_accuracy did not improve from 0.74646\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7521 - loss: 0.7135 - val_accuracy: 0.7358 - val_loss: 0.7018\n",
      "Epoch 4/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7608 - loss: 0.6559\n",
      "Epoch 4: val_accuracy improved from 0.74646 to 0.76887, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7614 - loss: 0.6549 - val_accuracy: 0.7689 - val_loss: 0.6500\n",
      "Epoch 5/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7922 - loss: 0.5870\n",
      "Epoch 5: val_accuracy did not improve from 0.76887\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7924 - loss: 0.5865 - val_accuracy: 0.7677 - val_loss: 0.6260\n",
      "Epoch 6/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8254 - loss: 0.4986\n",
      "Epoch 6: val_accuracy improved from 0.76887 to 0.80778, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8251 - loss: 0.4992 - val_accuracy: 0.8078 - val_loss: 0.5561\n",
      "Epoch 7/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8416 - loss: 0.4808\n",
      "Epoch 7: val_accuracy did not improve from 0.80778\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8414 - loss: 0.4805 - val_accuracy: 0.7972 - val_loss: 0.5304\n",
      "Epoch 8/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8519 - loss: 0.4228\n",
      "Epoch 8: val_accuracy improved from 0.80778 to 0.81604, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8518 - loss: 0.4230 - val_accuracy: 0.8160 - val_loss: 0.5060\n",
      "Epoch 9/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8423 - loss: 0.4368\n",
      "Epoch 9: val_accuracy improved from 0.81604 to 0.82901, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8423 - loss: 0.4367 - val_accuracy: 0.8290 - val_loss: 0.4994\n",
      "Epoch 10/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8460 - loss: 0.4131\n",
      "Epoch 10: val_accuracy did not improve from 0.82901\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8467 - loss: 0.4118 - val_accuracy: 0.8137 - val_loss: 0.5220\n",
      "Epoch 11/200\n",
      "\u001b[1m75/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8626 - loss: 0.3686\n",
      "Epoch 11: val_accuracy improved from 0.82901 to 0.84552, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8626 - loss: 0.3684 - val_accuracy: 0.8455 - val_loss: 0.4497\n",
      "Epoch 12/200\n",
      "\u001b[1m75/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8834 - loss: 0.3296\n",
      "Epoch 12: val_accuracy did not improve from 0.84552\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8830 - loss: 0.3307 - val_accuracy: 0.8408 - val_loss: 0.4534\n",
      "Epoch 13/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8835 - loss: 0.3094\n",
      "Epoch 13: val_accuracy did not improve from 0.84552\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8832 - loss: 0.3098 - val_accuracy: 0.8420 - val_loss: 0.4359\n",
      "Epoch 14/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8929 - loss: 0.2933\n",
      "Epoch 14: val_accuracy did not improve from 0.84552\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8926 - loss: 0.2942 - val_accuracy: 0.8208 - val_loss: 0.4638\n",
      "Epoch 15/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9044 - loss: 0.2730\n",
      "Epoch 15: val_accuracy did not improve from 0.84552\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9044 - loss: 0.2731 - val_accuracy: 0.8443 - val_loss: 0.4442\n",
      "Epoch 16/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9208 - loss: 0.2235\n",
      "Epoch 16: val_accuracy did not improve from 0.84552\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9206 - loss: 0.2241 - val_accuracy: 0.8408 - val_loss: 0.4576\n",
      "Epoch 17/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9110 - loss: 0.2481\n",
      "Epoch 17: val_accuracy improved from 0.84552 to 0.85259, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9109 - loss: 0.2482 - val_accuracy: 0.8526 - val_loss: 0.4623\n",
      "Epoch 18/200\n",
      "\u001b[1m75/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9137 - loss: 0.2414\n",
      "Epoch 18: val_accuracy did not improve from 0.85259\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9137 - loss: 0.2414 - val_accuracy: 0.8420 - val_loss: 0.4346\n",
      "Epoch 19/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9214 - loss: 0.2214\n",
      "Epoch 19: val_accuracy improved from 0.85259 to 0.85731, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9213 - loss: 0.2214 - val_accuracy: 0.8573 - val_loss: 0.4500\n",
      "Epoch 20/200\n",
      "\u001b[1m75/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9408 - loss: 0.1888\n",
      "Epoch 20: val_accuracy did not improve from 0.85731\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9401 - loss: 0.1902 - val_accuracy: 0.8479 - val_loss: 0.4732\n",
      "Epoch 21/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9249 - loss: 0.2014\n",
      "Epoch 21: val_accuracy did not improve from 0.85731\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9252 - loss: 0.2013 - val_accuracy: 0.8443 - val_loss: 0.4710\n",
      "Epoch 22/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9389 - loss: 0.1623\n",
      "Epoch 22: val_accuracy did not improve from 0.85731\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9388 - loss: 0.1624 - val_accuracy: 0.8491 - val_loss: 0.4534\n",
      "Epoch 23/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9446 - loss: 0.1575\n",
      "Epoch 23: val_accuracy did not improve from 0.85731\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9441 - loss: 0.1590 - val_accuracy: 0.8255 - val_loss: 0.4905\n",
      "Epoch 24/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9402 - loss: 0.1731\n",
      "Epoch 24: val_accuracy improved from 0.85731 to 0.85967, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9404 - loss: 0.1725 - val_accuracy: 0.8597 - val_loss: 0.4166\n",
      "Epoch 25/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9549 - loss: 0.1332\n",
      "Epoch 25: val_accuracy improved from 0.85967 to 0.86085, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9548 - loss: 0.1335 - val_accuracy: 0.8608 - val_loss: 0.4150\n",
      "Epoch 26/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9518 - loss: 0.1521\n",
      "Epoch 26: val_accuracy did not improve from 0.86085\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9518 - loss: 0.1521 - val_accuracy: 0.8550 - val_loss: 0.4846\n",
      "Epoch 27/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9346 - loss: 0.1859\n",
      "Epoch 27: val_accuracy did not improve from 0.86085\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9345 - loss: 0.1862 - val_accuracy: 0.8479 - val_loss: 0.4834\n",
      "Epoch 28/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9491 - loss: 0.1570\n",
      "Epoch 28: val_accuracy did not improve from 0.86085\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9493 - loss: 0.1561 - val_accuracy: 0.8491 - val_loss: 0.4637\n",
      "Epoch 29/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9586 - loss: 0.1260\n",
      "Epoch 29: val_accuracy improved from 0.86085 to 0.86910, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9586 - loss: 0.1258 - val_accuracy: 0.8691 - val_loss: 0.4384\n",
      "Epoch 30/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9620 - loss: 0.1129\n",
      "Epoch 30: val_accuracy did not improve from 0.86910\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9621 - loss: 0.1128 - val_accuracy: 0.8656 - val_loss: 0.4664\n",
      "Epoch 31/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9661 - loss: 0.0941\n",
      "Epoch 31: val_accuracy did not improve from 0.86910\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9661 - loss: 0.0941 - val_accuracy: 0.8550 - val_loss: 0.4827\n",
      "Epoch 32/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9567 - loss: 0.1334\n",
      "Epoch 32: val_accuracy did not improve from 0.86910\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9568 - loss: 0.1333 - val_accuracy: 0.8373 - val_loss: 0.5830\n",
      "Epoch 33/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9595 - loss: 0.1213\n",
      "Epoch 33: val_accuracy did not improve from 0.86910\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9594 - loss: 0.1215 - val_accuracy: 0.8656 - val_loss: 0.4508\n",
      "Epoch 34/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9749 - loss: 0.0875\n",
      "Epoch 34: val_accuracy improved from 0.86910 to 0.87146, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9747 - loss: 0.0878 - val_accuracy: 0.8715 - val_loss: 0.4711\n",
      "Epoch 35/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9656 - loss: 0.0950\n",
      "Epoch 35: val_accuracy improved from 0.87146 to 0.87500, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9657 - loss: 0.0948 - val_accuracy: 0.8750 - val_loss: 0.4431\n",
      "Epoch 36/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9756 - loss: 0.0755\n",
      "Epoch 36: val_accuracy did not improve from 0.87500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9754 - loss: 0.0759 - val_accuracy: 0.8691 - val_loss: 0.4504\n",
      "Epoch 37/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9729 - loss: 0.0703\n",
      "Epoch 37: val_accuracy did not improve from 0.87500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9728 - loss: 0.0706 - val_accuracy: 0.8715 - val_loss: 0.4468\n",
      "Epoch 38/200\n",
      "\u001b[1m75/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9752 - loss: 0.0773\n",
      "Epoch 38: val_accuracy did not improve from 0.87500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9749 - loss: 0.0775 - val_accuracy: 0.8750 - val_loss: 0.4737\n",
      "Epoch 39/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9748 - loss: 0.0657\n",
      "Epoch 39: val_accuracy did not improve from 0.87500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9748 - loss: 0.0658 - val_accuracy: 0.8691 - val_loss: 0.4821\n",
      "Epoch 40/200\n",
      "\u001b[1m75/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9762 - loss: 0.0674\n",
      "Epoch 40: val_accuracy did not improve from 0.87500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9762 - loss: 0.0675 - val_accuracy: 0.8550 - val_loss: 0.5139\n",
      "Epoch 41/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9773 - loss: 0.0723\n",
      "Epoch 41: val_accuracy did not improve from 0.87500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9775 - loss: 0.0719 - val_accuracy: 0.8726 - val_loss: 0.4743\n",
      "Epoch 42/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9770 - loss: 0.0724\n",
      "Epoch 42: val_accuracy did not improve from 0.87500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9769 - loss: 0.0722 - val_accuracy: 0.8491 - val_loss: 0.5363\n",
      "Epoch 43/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9820 - loss: 0.0537\n",
      "Epoch 43: val_accuracy did not improve from 0.87500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9820 - loss: 0.0538 - val_accuracy: 0.8632 - val_loss: 0.4919\n",
      "Epoch 44/200\n",
      "\u001b[1m75/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9828 - loss: 0.0539\n",
      "Epoch 44: val_accuracy did not improve from 0.87500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9823 - loss: 0.0547 - val_accuracy: 0.8715 - val_loss: 0.4728\n",
      "Epoch 45/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9864 - loss: 0.0485\n",
      "Epoch 45: val_accuracy improved from 0.87500 to 0.87618, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9863 - loss: 0.0486 - val_accuracy: 0.8762 - val_loss: 0.5468\n",
      "Epoch 46/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9790 - loss: 0.0687\n",
      "Epoch 46: val_accuracy did not improve from 0.87618\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9790 - loss: 0.0687 - val_accuracy: 0.8738 - val_loss: 0.5204\n",
      "Epoch 47/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9732 - loss: 0.0932\n",
      "Epoch 47: val_accuracy did not improve from 0.87618\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9732 - loss: 0.0930 - val_accuracy: 0.8738 - val_loss: 0.5015\n",
      "Epoch 48/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9746 - loss: 0.0811\n",
      "Epoch 48: val_accuracy did not improve from 0.87618\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9747 - loss: 0.0806 - val_accuracy: 0.8502 - val_loss: 0.5609\n",
      "Epoch 49/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9640 - loss: 0.0950\n",
      "Epoch 49: val_accuracy did not improve from 0.87618\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9642 - loss: 0.0944 - val_accuracy: 0.8656 - val_loss: 0.5046\n",
      "Epoch 50/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9740 - loss: 0.0686\n",
      "Epoch 50: val_accuracy did not improve from 0.87618\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9740 - loss: 0.0685 - val_accuracy: 0.8691 - val_loss: 0.5042\n",
      "Epoch 51/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9796 - loss: 0.0582\n",
      "Epoch 51: val_accuracy did not improve from 0.87618\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9799 - loss: 0.0577 - val_accuracy: 0.8608 - val_loss: 0.5194\n",
      "Epoch 52/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9806 - loss: 0.0601\n",
      "Epoch 52: val_accuracy improved from 0.87618 to 0.87854, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9806 - loss: 0.0602 - val_accuracy: 0.8785 - val_loss: 0.4826\n",
      "Epoch 53/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9891 - loss: 0.0379\n",
      "Epoch 53: val_accuracy did not improve from 0.87854\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9890 - loss: 0.0382 - val_accuracy: 0.8715 - val_loss: 0.5430\n",
      "Epoch 54/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9859 - loss: 0.0518\n",
      "Epoch 54: val_accuracy did not improve from 0.87854\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9859 - loss: 0.0517 - val_accuracy: 0.8726 - val_loss: 0.5267\n",
      "Epoch 55/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9906 - loss: 0.0297\n",
      "Epoch 55: val_accuracy improved from 0.87854 to 0.89033, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9906 - loss: 0.0300 - val_accuracy: 0.8903 - val_loss: 0.4838\n",
      "Epoch 56/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9848 - loss: 0.0641\n",
      "Epoch 56: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9848 - loss: 0.0636 - val_accuracy: 0.8726 - val_loss: 0.5221\n",
      "Epoch 57/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9726 - loss: 0.0819\n",
      "Epoch 57: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9729 - loss: 0.0810 - val_accuracy: 0.8726 - val_loss: 0.5304\n",
      "Epoch 58/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9896 - loss: 0.0327\n",
      "Epoch 58: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9896 - loss: 0.0328 - val_accuracy: 0.8679 - val_loss: 0.5110\n",
      "Epoch 59/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9907 - loss: 0.0301\n",
      "Epoch 59: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9907 - loss: 0.0302 - val_accuracy: 0.8715 - val_loss: 0.5201\n",
      "Epoch 60/200\n",
      "\u001b[1m75/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9927 - loss: 0.0300\n",
      "Epoch 60: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9925 - loss: 0.0304 - val_accuracy: 0.8833 - val_loss: 0.5011\n",
      "Epoch 61/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9932 - loss: 0.0228\n",
      "Epoch 61: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9932 - loss: 0.0228 - val_accuracy: 0.8880 - val_loss: 0.5363\n",
      "Epoch 62/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9930 - loss: 0.0254\n",
      "Epoch 62: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9928 - loss: 0.0261 - val_accuracy: 0.8715 - val_loss: 0.5812\n",
      "Epoch 63/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9777 - loss: 0.0648\n",
      "Epoch 63: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9780 - loss: 0.0648 - val_accuracy: 0.8620 - val_loss: 0.5613\n",
      "Epoch 64/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9822 - loss: 0.0577\n",
      "Epoch 64: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9822 - loss: 0.0580 - val_accuracy: 0.8750 - val_loss: 0.5389\n",
      "Epoch 65/200\n",
      "\u001b[1m75/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9843 - loss: 0.0483\n",
      "Epoch 65: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9842 - loss: 0.0485 - val_accuracy: 0.8526 - val_loss: 0.5660\n",
      "Epoch 66/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9897 - loss: 0.0330\n",
      "Epoch 66: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9897 - loss: 0.0329 - val_accuracy: 0.8679 - val_loss: 0.5874\n",
      "Epoch 67/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9924 - loss: 0.0284\n",
      "Epoch 67: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9923 - loss: 0.0285 - val_accuracy: 0.8691 - val_loss: 0.5795\n",
      "Epoch 68/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9880 - loss: 0.0305\n",
      "Epoch 68: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9880 - loss: 0.0306 - val_accuracy: 0.8762 - val_loss: 0.5256\n",
      "Epoch 69/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9936 - loss: 0.0252\n",
      "Epoch 69: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9936 - loss: 0.0252 - val_accuracy: 0.8821 - val_loss: 0.5587\n",
      "Epoch 70/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9951 - loss: 0.0179\n",
      "Epoch 70: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9951 - loss: 0.0180 - val_accuracy: 0.8750 - val_loss: 0.5593\n",
      "Epoch 71/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9920 - loss: 0.0247\n",
      "Epoch 71: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9919 - loss: 0.0251 - val_accuracy: 0.8797 - val_loss: 0.5718\n",
      "Epoch 72/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9818 - loss: 0.0662\n",
      "Epoch 72: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9818 - loss: 0.0661 - val_accuracy: 0.8656 - val_loss: 0.5385\n",
      "Epoch 73/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9904 - loss: 0.0310\n",
      "Epoch 73: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9904 - loss: 0.0312 - val_accuracy: 0.8656 - val_loss: 0.5870\n",
      "Epoch 74/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9860 - loss: 0.0445\n",
      "Epoch 74: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9860 - loss: 0.0445 - val_accuracy: 0.8844 - val_loss: 0.5088\n",
      "Epoch 75/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9899 - loss: 0.0306\n",
      "Epoch 75: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9901 - loss: 0.0306 - val_accuracy: 0.8738 - val_loss: 0.5505\n",
      "Epoch 76/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9852 - loss: 0.0397\n",
      "Epoch 76: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9852 - loss: 0.0397 - val_accuracy: 0.8821 - val_loss: 0.5225\n",
      "Epoch 77/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9859 - loss: 0.0308\n",
      "Epoch 77: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9860 - loss: 0.0306 - val_accuracy: 0.8774 - val_loss: 0.5534\n",
      "Epoch 78/200\n",
      "\u001b[1m74/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9879 - loss: 0.0305\n",
      "Epoch 78: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9877 - loss: 0.0310 - val_accuracy: 0.8703 - val_loss: 0.6354\n",
      "Epoch 79/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9911 - loss: 0.0262\n",
      "Epoch 79: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9912 - loss: 0.0262 - val_accuracy: 0.8715 - val_loss: 0.6367\n",
      "Epoch 80/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9953 - loss: 0.0165\n",
      "Epoch 80: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9953 - loss: 0.0167 - val_accuracy: 0.8785 - val_loss: 0.5866\n",
      "Epoch 81/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9922 - loss: 0.0202\n",
      "Epoch 81: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9922 - loss: 0.0202 - val_accuracy: 0.8785 - val_loss: 0.5915\n",
      "Epoch 82/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9932 - loss: 0.0223\n",
      "Epoch 82: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9932 - loss: 0.0224 - val_accuracy: 0.8762 - val_loss: 0.5450\n",
      "Epoch 83/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9868 - loss: 0.0379\n",
      "Epoch 83: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9868 - loss: 0.0380 - val_accuracy: 0.8821 - val_loss: 0.5500\n",
      "Epoch 84/200\n",
      "\u001b[1m75/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9924 - loss: 0.0287\n",
      "Epoch 84: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9922 - loss: 0.0290 - val_accuracy: 0.8726 - val_loss: 0.5800\n",
      "Epoch 85/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9856 - loss: 0.0401\n",
      "Epoch 85: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9857 - loss: 0.0399 - val_accuracy: 0.8667 - val_loss: 0.5763\n",
      "Epoch 86/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9820 - loss: 0.0494\n",
      "Epoch 86: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9821 - loss: 0.0492 - val_accuracy: 0.8738 - val_loss: 0.5589\n",
      "Epoch 87/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9975 - loss: 0.0175\n",
      "Epoch 87: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9973 - loss: 0.0177 - val_accuracy: 0.8821 - val_loss: 0.6072\n",
      "Epoch 88/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9907 - loss: 0.0251 \n",
      "Epoch 88: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9908 - loss: 0.0250 - val_accuracy: 0.8785 - val_loss: 0.5721\n",
      "Epoch 89/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9908 - loss: 0.0261\n",
      "Epoch 89: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9908 - loss: 0.0261 - val_accuracy: 0.8809 - val_loss: 0.5928\n",
      "Epoch 90/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9939 - loss: 0.0204\n",
      "Epoch 90: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9936 - loss: 0.0209 - val_accuracy: 0.8844 - val_loss: 0.5925\n",
      "Epoch 91/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9915 - loss: 0.0235\n",
      "Epoch 91: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9915 - loss: 0.0237 - val_accuracy: 0.8632 - val_loss: 0.6365\n",
      "Epoch 92/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9918 - loss: 0.0185\n",
      "Epoch 92: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9918 - loss: 0.0186 - val_accuracy: 0.8750 - val_loss: 0.6221\n",
      "Epoch 93/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9933 - loss: 0.0244\n",
      "Epoch 93: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9931 - loss: 0.0247 - val_accuracy: 0.8644 - val_loss: 0.6863\n",
      "Epoch 94/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9846 - loss: 0.0371\n",
      "Epoch 94: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9848 - loss: 0.0370 - val_accuracy: 0.8632 - val_loss: 0.7298\n",
      "Epoch 95/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9906 - loss: 0.0263\n",
      "Epoch 95: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9906 - loss: 0.0262 - val_accuracy: 0.8762 - val_loss: 0.6080\n",
      "Epoch 96/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9950 - loss: 0.0124\n",
      "Epoch 96: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9950 - loss: 0.0124 - val_accuracy: 0.8809 - val_loss: 0.5826\n",
      "Epoch 97/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9976 - loss: 0.0098\n",
      "Epoch 97: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9975 - loss: 0.0100 - val_accuracy: 0.8750 - val_loss: 0.6138\n",
      "Epoch 98/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9875 - loss: 0.0387\n",
      "Epoch 98: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9876 - loss: 0.0384 - val_accuracy: 0.8892 - val_loss: 0.5441\n",
      "Epoch 99/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9958 - loss: 0.0132\n",
      "Epoch 99: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9957 - loss: 0.0134 - val_accuracy: 0.8833 - val_loss: 0.5864\n",
      "Epoch 100/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9917 - loss: 0.0326\n",
      "Epoch 100: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9917 - loss: 0.0325 - val_accuracy: 0.8691 - val_loss: 0.6358\n",
      "Epoch 101/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9886 - loss: 0.0329\n",
      "Epoch 101: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9886 - loss: 0.0329 - val_accuracy: 0.8715 - val_loss: 0.5757\n",
      "Epoch 102/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9906 - loss: 0.0235\n",
      "Epoch 102: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9905 - loss: 0.0236 - val_accuracy: 0.8762 - val_loss: 0.6599\n",
      "Epoch 103/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9933 - loss: 0.0214\n",
      "Epoch 103: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9933 - loss: 0.0213 - val_accuracy: 0.8738 - val_loss: 0.6045\n",
      "Epoch 104/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9896 - loss: 0.0282\n",
      "Epoch 104: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9896 - loss: 0.0282 - val_accuracy: 0.8715 - val_loss: 0.6575\n",
      "Epoch 105/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9914 - loss: 0.0267\n",
      "Epoch 105: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9915 - loss: 0.0265 - val_accuracy: 0.8726 - val_loss: 0.6433\n",
      "Epoch 106/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9870 - loss: 0.0472\n",
      "Epoch 106: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9870 - loss: 0.0467 - val_accuracy: 0.8750 - val_loss: 0.6137\n",
      "Epoch 107/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9905 - loss: 0.0275\n",
      "Epoch 107: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9905 - loss: 0.0275 - val_accuracy: 0.8785 - val_loss: 0.6191\n",
      "Epoch 108/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9934 - loss: 0.0230\n",
      "Epoch 108: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9933 - loss: 0.0231 - val_accuracy: 0.8726 - val_loss: 0.6271\n",
      "Epoch 109/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9841 - loss: 0.0652\n",
      "Epoch 109: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9840 - loss: 0.0652 - val_accuracy: 0.8679 - val_loss: 0.5723\n",
      "Epoch 110/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9901 - loss: 0.0327\n",
      "Epoch 110: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9902 - loss: 0.0326 - val_accuracy: 0.8809 - val_loss: 0.5423\n",
      "Epoch 111/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9953 - loss: 0.0119\n",
      "Epoch 111: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9953 - loss: 0.0119 - val_accuracy: 0.8821 - val_loss: 0.5607\n",
      "Epoch 112/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9903 - loss: 0.0263\n",
      "Epoch 112: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9904 - loss: 0.0262 - val_accuracy: 0.8762 - val_loss: 0.6203\n",
      "Epoch 113/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9925 - loss: 0.0218\n",
      "Epoch 113: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9924 - loss: 0.0218 - val_accuracy: 0.8738 - val_loss: 0.5836\n",
      "Epoch 114/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9937 - loss: 0.0148\n",
      "Epoch 114: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9938 - loss: 0.0148 - val_accuracy: 0.8797 - val_loss: 0.5760\n",
      "Epoch 115/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9963 - loss: 0.0116\n",
      "Epoch 115: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9963 - loss: 0.0117 - val_accuracy: 0.8644 - val_loss: 0.7522\n",
      "Epoch 116/200\n",
      "\u001b[1m75/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9934 - loss: 0.0225\n",
      "Epoch 116: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9933 - loss: 0.0226 - val_accuracy: 0.8762 - val_loss: 0.6401\n",
      "Epoch 117/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9949 - loss: 0.0192\n",
      "Epoch 117: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9949 - loss: 0.0192 - val_accuracy: 0.8774 - val_loss: 0.6171\n",
      "Epoch 118/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9933 - loss: 0.0183\n",
      "Epoch 118: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9933 - loss: 0.0183 - val_accuracy: 0.8632 - val_loss: 0.7500\n",
      "Epoch 119/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9906 - loss: 0.0228\n",
      "Epoch 119: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9906 - loss: 0.0231 - val_accuracy: 0.8656 - val_loss: 0.6616\n",
      "Epoch 120/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9940 - loss: 0.0214\n",
      "Epoch 120: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9940 - loss: 0.0213 - val_accuracy: 0.8762 - val_loss: 0.6091\n",
      "Epoch 121/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9976 - loss: 0.0087\n",
      "Epoch 121: val_accuracy did not improve from 0.89033\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9975 - loss: 0.0089 - val_accuracy: 0.8785 - val_loss: 0.6516\n",
      "Epoch 122/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9953 - loss: 0.0159\n",
      "Epoch 122: val_accuracy improved from 0.89033 to 0.89151, saving model to best_model.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9952 - loss: 0.0160 - val_accuracy: 0.8915 - val_loss: 0.6041\n",
      "Epoch 123/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9941 - loss: 0.0147\n",
      "Epoch 123: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9940 - loss: 0.0148 - val_accuracy: 0.8703 - val_loss: 0.6926\n",
      "Epoch 124/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9931 - loss: 0.0227\n",
      "Epoch 124: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9931 - loss: 0.0226 - val_accuracy: 0.8809 - val_loss: 0.6146\n",
      "Epoch 125/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9926 - loss: 0.0185\n",
      "Epoch 125: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9926 - loss: 0.0186 - val_accuracy: 0.8868 - val_loss: 0.6143\n",
      "Epoch 126/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9917 - loss: 0.0233\n",
      "Epoch 126: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9917 - loss: 0.0233 - val_accuracy: 0.8868 - val_loss: 0.6374\n",
      "Epoch 127/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9950 - loss: 0.0183\n",
      "Epoch 127: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9949 - loss: 0.0185 - val_accuracy: 0.8715 - val_loss: 0.6444\n",
      "Epoch 128/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9887 - loss: 0.0335\n",
      "Epoch 128: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9888 - loss: 0.0335 - val_accuracy: 0.8738 - val_loss: 0.6251\n",
      "Epoch 129/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9966 - loss: 0.0129\n",
      "Epoch 129: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9966 - loss: 0.0129 - val_accuracy: 0.8726 - val_loss: 0.6773\n",
      "Epoch 130/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9956 - loss: 0.0097\n",
      "Epoch 130: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9957 - loss: 0.0097 - val_accuracy: 0.8809 - val_loss: 0.6560\n",
      "Epoch 131/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9964 - loss: 0.0109\n",
      "Epoch 131: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9965 - loss: 0.0108 - val_accuracy: 0.8915 - val_loss: 0.6290\n",
      "Epoch 132/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9947 - loss: 0.0210\n",
      "Epoch 132: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9945 - loss: 0.0216 - val_accuracy: 0.8762 - val_loss: 0.7500\n",
      "Epoch 133/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9815 - loss: 0.0588\n",
      "Epoch 133: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9817 - loss: 0.0580 - val_accuracy: 0.8738 - val_loss: 0.6390\n",
      "Epoch 134/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9905 - loss: 0.0208\n",
      "Epoch 134: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9906 - loss: 0.0208 - val_accuracy: 0.8644 - val_loss: 0.7053\n",
      "Epoch 135/200\n",
      "\u001b[1m75/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9906 - loss: 0.0297\n",
      "Epoch 135: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9906 - loss: 0.0298 - val_accuracy: 0.8656 - val_loss: 0.6667\n",
      "Epoch 136/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9936 - loss: 0.0142\n",
      "Epoch 136: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9935 - loss: 0.0146 - val_accuracy: 0.8691 - val_loss: 0.6653\n",
      "Epoch 137/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9962 - loss: 0.0197\n",
      "Epoch 137: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9962 - loss: 0.0197 - val_accuracy: 0.8679 - val_loss: 0.7018\n",
      "Epoch 138/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9959 - loss: 0.0118\n",
      "Epoch 138: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9959 - loss: 0.0118 - val_accuracy: 0.8785 - val_loss: 0.6790\n",
      "Epoch 139/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9977 - loss: 0.0066\n",
      "Epoch 139: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9976 - loss: 0.0068 - val_accuracy: 0.8797 - val_loss: 0.6843\n",
      "Epoch 140/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9978 - loss: 0.0092\n",
      "Epoch 140: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9977 - loss: 0.0094 - val_accuracy: 0.8809 - val_loss: 0.7303\n",
      "Epoch 141/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9957 - loss: 0.0147\n",
      "Epoch 141: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9956 - loss: 0.0150 - val_accuracy: 0.8691 - val_loss: 0.7627\n",
      "Epoch 142/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9913 - loss: 0.0228\n",
      "Epoch 142: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9914 - loss: 0.0228 - val_accuracy: 0.8785 - val_loss: 0.6760\n",
      "Epoch 143/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9975 - loss: 0.0076\n",
      "Epoch 143: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9974 - loss: 0.0078 - val_accuracy: 0.8738 - val_loss: 0.6580\n",
      "Epoch 144/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9971 - loss: 0.0072\n",
      "Epoch 144: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9972 - loss: 0.0072 - val_accuracy: 0.8774 - val_loss: 0.6665\n",
      "Epoch 145/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9950 - loss: 0.0202\n",
      "Epoch 145: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9950 - loss: 0.0201 - val_accuracy: 0.8608 - val_loss: 0.7515\n",
      "Epoch 146/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9886 - loss: 0.0356\n",
      "Epoch 146: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9887 - loss: 0.0354 - val_accuracy: 0.8762 - val_loss: 0.7141\n",
      "Epoch 147/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9833 - loss: 0.0747\n",
      "Epoch 147: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9832 - loss: 0.0748 - val_accuracy: 0.8644 - val_loss: 0.7427\n",
      "Epoch 148/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9895 - loss: 0.0293\n",
      "Epoch 148: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9895 - loss: 0.0292 - val_accuracy: 0.8856 - val_loss: 0.5995\n",
      "Epoch 149/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9964 - loss: 0.0126\n",
      "Epoch 149: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9963 - loss: 0.0127 - val_accuracy: 0.8667 - val_loss: 0.7045\n",
      "Epoch 150/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9918 - loss: 0.0248\n",
      "Epoch 150: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9918 - loss: 0.0247 - val_accuracy: 0.8691 - val_loss: 0.6827\n",
      "Epoch 151/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9966 - loss: 0.0101\n",
      "Epoch 151: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9966 - loss: 0.0102 - val_accuracy: 0.8833 - val_loss: 0.6689\n",
      "Epoch 152/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9997 - loss: 0.0047\n",
      "Epoch 152: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9997 - loss: 0.0048 - val_accuracy: 0.8774 - val_loss: 0.7193\n",
      "Epoch 153/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9972 - loss: 0.0082\n",
      "Epoch 153: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9971 - loss: 0.0083 - val_accuracy: 0.8833 - val_loss: 0.6572\n",
      "Epoch 154/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9957 - loss: 0.0193\n",
      "Epoch 154: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9957 - loss: 0.0190 - val_accuracy: 0.8738 - val_loss: 0.6753\n",
      "Epoch 155/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9951 - loss: 0.0149\n",
      "Epoch 155: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9951 - loss: 0.0148 - val_accuracy: 0.8892 - val_loss: 0.6606\n",
      "Epoch 156/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9928 - loss: 0.0162\n",
      "Epoch 156: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9928 - loss: 0.0163 - val_accuracy: 0.8785 - val_loss: 0.7097\n",
      "Epoch 157/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9972 - loss: 0.0090\n",
      "Epoch 157: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9972 - loss: 0.0089 - val_accuracy: 0.8844 - val_loss: 0.6949\n",
      "Epoch 158/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9959 - loss: 0.0108\n",
      "Epoch 158: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.9959 - loss: 0.0110 - val_accuracy: 0.8797 - val_loss: 0.6787\n",
      "Epoch 159/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9961 - loss: 0.0133\n",
      "Epoch 159: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9960 - loss: 0.0133 - val_accuracy: 0.8833 - val_loss: 0.6832\n",
      "Epoch 160/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9963 - loss: 0.0107\n",
      "Epoch 160: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9963 - loss: 0.0108 - val_accuracy: 0.8726 - val_loss: 0.7299\n",
      "Epoch 161/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9908 - loss: 0.0262\n",
      "Epoch 161: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9908 - loss: 0.0263 - val_accuracy: 0.8738 - val_loss: 0.6811\n",
      "Epoch 162/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9961 - loss: 0.0159\n",
      "Epoch 162: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9961 - loss: 0.0159 - val_accuracy: 0.8715 - val_loss: 0.6491\n",
      "Epoch 163/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9980 - loss: 0.0071\n",
      "Epoch 163: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9980 - loss: 0.0071 - val_accuracy: 0.8679 - val_loss: 0.7524\n",
      "Epoch 164/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9899 - loss: 0.0275\n",
      "Epoch 164: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9899 - loss: 0.0276 - val_accuracy: 0.8750 - val_loss: 0.7149\n",
      "Epoch 165/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9948 - loss: 0.0239\n",
      "Epoch 165: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9947 - loss: 0.0240 - val_accuracy: 0.8821 - val_loss: 0.6361\n",
      "Epoch 166/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9902 - loss: 0.0217\n",
      "Epoch 166: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9902 - loss: 0.0217 - val_accuracy: 0.8797 - val_loss: 0.6969\n",
      "Epoch 167/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9933 - loss: 0.0192\n",
      "Epoch 167: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9933 - loss: 0.0192 - val_accuracy: 0.8868 - val_loss: 0.6000\n",
      "Epoch 168/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9961 - loss: 0.0104\n",
      "Epoch 168: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9961 - loss: 0.0104 - val_accuracy: 0.8762 - val_loss: 0.6593\n",
      "Epoch 169/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9911 - loss: 0.0212\n",
      "Epoch 169: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9912 - loss: 0.0211 - val_accuracy: 0.8750 - val_loss: 0.6994\n",
      "Epoch 170/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9986 - loss: 0.0064\n",
      "Epoch 170: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0064 - val_accuracy: 0.8785 - val_loss: 0.6969\n",
      "Epoch 171/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9966 - loss: 0.0119\n",
      "Epoch 171: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9966 - loss: 0.0118 - val_accuracy: 0.8785 - val_loss: 0.7019\n",
      "Epoch 172/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9995 - loss: 0.0030\n",
      "Epoch 172: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.9995 - loss: 0.0032 - val_accuracy: 0.8880 - val_loss: 0.6632\n",
      "Epoch 173/200\n",
      "\u001b[1m75/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9944 - loss: 0.0139\n",
      "Epoch 173: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9943 - loss: 0.0140 - val_accuracy: 0.8750 - val_loss: 0.7355\n",
      "Epoch 174/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9947 - loss: 0.0133\n",
      "Epoch 174: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9946 - loss: 0.0133 - val_accuracy: 0.8738 - val_loss: 0.7413\n",
      "Epoch 175/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9973 - loss: 0.0089\n",
      "Epoch 175: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9972 - loss: 0.0091 - val_accuracy: 0.8821 - val_loss: 0.7188\n",
      "Epoch 176/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9921 - loss: 0.0381\n",
      "Epoch 176: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9921 - loss: 0.0381 - val_accuracy: 0.8750 - val_loss: 0.6691\n",
      "Epoch 177/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9883 - loss: 0.0344\n",
      "Epoch 177: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9884 - loss: 0.0343 - val_accuracy: 0.8762 - val_loss: 0.7371\n",
      "Epoch 178/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9960 - loss: 0.0146\n",
      "Epoch 178: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9959 - loss: 0.0147 - val_accuracy: 0.8821 - val_loss: 0.6782\n",
      "Epoch 179/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9972 - loss: 0.0143\n",
      "Epoch 179: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9972 - loss: 0.0144 - val_accuracy: 0.8726 - val_loss: 0.6741\n",
      "Epoch 180/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9971 - loss: 0.0119\n",
      "Epoch 180: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9971 - loss: 0.0119 - val_accuracy: 0.8785 - val_loss: 0.6578\n",
      "Epoch 181/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9975 - loss: 0.0104\n",
      "Epoch 181: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9974 - loss: 0.0106 - val_accuracy: 0.8726 - val_loss: 0.7200\n",
      "Epoch 182/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9950 - loss: 0.0199\n",
      "Epoch 182: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9950 - loss: 0.0199 - val_accuracy: 0.8797 - val_loss: 0.7091\n",
      "Epoch 183/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0072 \n",
      "Epoch 183: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9980 - loss: 0.0071 - val_accuracy: 0.8809 - val_loss: 0.6731\n",
      "Epoch 184/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9969 - loss: 0.0079\n",
      "Epoch 184: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9969 - loss: 0.0079 - val_accuracy: 0.8703 - val_loss: 0.7208\n",
      "Epoch 185/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9995 - loss: 0.0041\n",
      "Epoch 185: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.9995 - loss: 0.0041 - val_accuracy: 0.8833 - val_loss: 0.6559\n",
      "Epoch 186/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9968 - loss: 0.0083\n",
      "Epoch 186: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9969 - loss: 0.0083 - val_accuracy: 0.8762 - val_loss: 0.6793\n",
      "Epoch 187/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9958 - loss: 0.0099\n",
      "Epoch 187: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.9958 - loss: 0.0102 - val_accuracy: 0.8750 - val_loss: 0.7138\n",
      "Epoch 188/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9905 - loss: 0.0268\n",
      "Epoch 188: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.9905 - loss: 0.0269 - val_accuracy: 0.8585 - val_loss: 0.7827\n",
      "Epoch 189/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9865 - loss: 0.0369\n",
      "Epoch 189: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.9865 - loss: 0.0369 - val_accuracy: 0.8632 - val_loss: 0.7444\n",
      "Epoch 190/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9947 - loss: 0.0151\n",
      "Epoch 190: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9946 - loss: 0.0153 - val_accuracy: 0.8620 - val_loss: 0.7383\n",
      "Epoch 191/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9838 - loss: 0.0422\n",
      "Epoch 191: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9838 - loss: 0.0420 - val_accuracy: 0.8785 - val_loss: 0.6680\n",
      "Epoch 192/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9946 - loss: 0.0209\n",
      "Epoch 192: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9947 - loss: 0.0208 - val_accuracy: 0.8691 - val_loss: 0.6314\n",
      "Epoch 193/200\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9957 - loss: 0.0109\n",
      "Epoch 193: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9957 - loss: 0.0109 - val_accuracy: 0.8750 - val_loss: 0.7296\n",
      "Epoch 194/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9949 - loss: 0.0111\n",
      "Epoch 194: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9949 - loss: 0.0110 - val_accuracy: 0.8703 - val_loss: 0.6412\n",
      "Epoch 195/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9989 - loss: 0.0042\n",
      "Epoch 195: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 0.8762 - val_loss: 0.6082\n",
      "Epoch 196/200\n",
      "\u001b[1m76/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9956 - loss: 0.0114\n",
      "Epoch 196: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9956 - loss: 0.0115 - val_accuracy: 0.8821 - val_loss: 0.6656\n",
      "Epoch 197/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0065\n",
      "Epoch 197: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9979 - loss: 0.0065 - val_accuracy: 0.8833 - val_loss: 0.6756\n",
      "Epoch 198/200\n",
      "\u001b[1m78/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9945 - loss: 0.0128\n",
      "Epoch 198: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9945 - loss: 0.0128 - val_accuracy: 0.8738 - val_loss: 0.7389\n",
      "Epoch 199/200\n",
      "\u001b[1m79/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9983 - loss: 0.0066\n",
      "Epoch 199: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9983 - loss: 0.0066 - val_accuracy: 0.8809 - val_loss: 0.7539\n",
      "Epoch 200/200\n",
      "\u001b[1m77/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9993 - loss: 0.0036\n",
      "Epoch 200: val_accuracy did not improve from 0.89151\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9993 - loss: 0.0038 - val_accuracy: 0.8656 - val_loss: 0.7627\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the filepath to save the best model\n",
    "filepath = \"best_model.keras\"\n",
    "\n",
    "# Define the ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "# Compile your model before training\n",
    "# model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train your model with the ModelCheckpoint callback\n",
    "history = model.fit(X_train, y_train_encoded, epochs=200, validation_data=(X_val, y_val_encoded), callbacks=[checkpoint])\n",
    "\n",
    "# After training, you can load the best model using:\n",
    "# best_model = keras.models.load_model(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8478 - loss: 1.0158\n",
      "Test Loss: 0.9382721781730652\n",
      "Test Accuracy: 0.8632075190544128\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Test Data accuracy:  86.32075471698113\n",
      "Test Data accuracy:  86.32075471698113\n",
      "Precision:  0.8643420140524252\n",
      "Recall:  0.8632075471698113\n",
      "F1 Score:  0.8630591074131948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "y_predict = np.argmax(model.predict(X_test), axis=-1)\n",
    "accuracy = accuracy_score(y_test_encoded, y_predict)\n",
    "precision = precision_score(y_test_encoded, y_predict, average='weighted')\n",
    "recall = recall_score(y_test_encoded, y_predict, average='weighted')\n",
    "f1 = f1_score(y_test_encoded, y_predict, average='weighted')\n",
    "print('Test Data accuracy: ',accuracy_score(y_test_encoded, y_predict)*100)\n",
    "print('Test Data accuracy: ', accuracy * 100)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1 Score: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "def lstm_cnn(input_shape, num_classes):\n",
    "# Define the combined model\n",
    "    model = Sequential()\n",
    "\n",
    "# Add a 1D CNN layer\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "# Add a 1D CNN layer\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "# Add a 1D CNN layer\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "# Add an LSTM layer\n",
    "    model.add(LSTM(units=64, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "# Flatten the output of the LSTM layer\n",
    "    model.add(Flatten())\n",
    "\n",
    "# Fully Connected (FC) layer\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Assuming 8 emotion classes\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\youss\\miniconda3\\envs\\speech\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">903</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_12 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_13 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m98,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_13 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_14 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m24,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_14 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_28 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_29 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m903\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">241,095</span> (941.78 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m241,095\u001b[0m (941.78 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">240,711</span> (940.28 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m240,711\u001b[0m (940.28 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define input shape and number of classes\n",
    "input_shape = (X_train.shape[1], 1)  # Each audio frame has 40 features\n",
    "num_classes = 7  # Number of classes (Neutral, Calm, Happy, Sad, Angry, Fearful, Disgust, Surprised)\n",
    "\n",
    "# Create the model\n",
    "model = lstm_cnn(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5111 - loss: 1.3331\n",
      "Epoch 1: val_accuracy improved from -inf to 0.75024, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 31ms/step - accuracy: 0.5113 - loss: 1.3324 - val_accuracy: 0.7502 - val_loss: 0.6864\n",
      "Epoch 2/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7236 - loss: 0.7587\n",
      "Epoch 2: val_accuracy improved from 0.75024 to 0.78231, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.7237 - loss: 0.7585 - val_accuracy: 0.7823 - val_loss: 0.5835\n",
      "Epoch 3/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7627 - loss: 0.6552\n",
      "Epoch 3: val_accuracy improved from 0.78231 to 0.80236, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.7627 - loss: 0.6551 - val_accuracy: 0.8024 - val_loss: 0.5367\n",
      "Epoch 4/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7855 - loss: 0.5954\n",
      "Epoch 4: val_accuracy improved from 0.80236 to 0.82005, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.7855 - loss: 0.5954 - val_accuracy: 0.8200 - val_loss: 0.4907\n",
      "Epoch 5/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7963 - loss: 0.5659\n",
      "Epoch 5: val_accuracy improved from 0.82005 to 0.83325, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.7963 - loss: 0.5659 - val_accuracy: 0.8333 - val_loss: 0.4728\n",
      "Epoch 6/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8019 - loss: 0.5383\n",
      "Epoch 6: val_accuracy improved from 0.83325 to 0.84104, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.8019 - loss: 0.5383 - val_accuracy: 0.8410 - val_loss: 0.4433\n",
      "Epoch 7/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8122 - loss: 0.5171\n",
      "Epoch 7: val_accuracy improved from 0.84104 to 0.85330, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.8122 - loss: 0.5171 - val_accuracy: 0.8533 - val_loss: 0.4099\n",
      "Epoch 8/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8219 - loss: 0.4935\n",
      "Epoch 8: val_accuracy did not improve from 0.85330\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.8219 - loss: 0.4935 - val_accuracy: 0.8493 - val_loss: 0.4066\n",
      "Epoch 9/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8254 - loss: 0.4787\n",
      "Epoch 9: val_accuracy did not improve from 0.85330\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.8254 - loss: 0.4787 - val_accuracy: 0.8519 - val_loss: 0.3969\n",
      "Epoch 10/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8364 - loss: 0.4575\n",
      "Epoch 10: val_accuracy improved from 0.85330 to 0.85873, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.8364 - loss: 0.4576 - val_accuracy: 0.8587 - val_loss: 0.3804\n",
      "Epoch 11/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8393 - loss: 0.4459\n",
      "Epoch 11: val_accuracy improved from 0.85873 to 0.86722, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.8393 - loss: 0.4460 - val_accuracy: 0.8672 - val_loss: 0.3752\n",
      "Epoch 12/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8378 - loss: 0.4413\n",
      "Epoch 12: val_accuracy improved from 0.86722 to 0.86816, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.8378 - loss: 0.4413 - val_accuracy: 0.8682 - val_loss: 0.3508\n",
      "Epoch 13/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8444 - loss: 0.4272\n",
      "Epoch 13: val_accuracy improved from 0.86816 to 0.87146, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.8444 - loss: 0.4273 - val_accuracy: 0.8715 - val_loss: 0.3471\n",
      "Epoch 14/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8457 - loss: 0.4207\n",
      "Epoch 14: val_accuracy improved from 0.87146 to 0.88420, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.8457 - loss: 0.4207 - val_accuracy: 0.8842 - val_loss: 0.3236\n",
      "Epoch 15/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8506 - loss: 0.4058\n",
      "Epoch 15: val_accuracy improved from 0.88420 to 0.88467, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.8506 - loss: 0.4058 - val_accuracy: 0.8847 - val_loss: 0.3260\n",
      "Epoch 16/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8533 - loss: 0.3991\n",
      "Epoch 16: val_accuracy improved from 0.88467 to 0.89080, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.8533 - loss: 0.3991 - val_accuracy: 0.8908 - val_loss: 0.3061\n",
      "Epoch 17/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8646 - loss: 0.3825\n",
      "Epoch 17: val_accuracy did not improve from 0.89080\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.8645 - loss: 0.3826 - val_accuracy: 0.8870 - val_loss: 0.3022\n",
      "Epoch 18/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8637 - loss: 0.3753\n",
      "Epoch 18: val_accuracy did not improve from 0.89080\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.8637 - loss: 0.3754 - val_accuracy: 0.8889 - val_loss: 0.2981\n",
      "Epoch 19/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8649 - loss: 0.3602\n",
      "Epoch 19: val_accuracy improved from 0.89080 to 0.89292, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.8649 - loss: 0.3603 - val_accuracy: 0.8929 - val_loss: 0.2898\n",
      "Epoch 20/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8740 - loss: 0.3465\n",
      "Epoch 20: val_accuracy improved from 0.89292 to 0.89953, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.8740 - loss: 0.3466 - val_accuracy: 0.8995 - val_loss: 0.2720\n",
      "Epoch 21/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8695 - loss: 0.3542\n",
      "Epoch 21: val_accuracy did not improve from 0.89953\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.8695 - loss: 0.3542 - val_accuracy: 0.8988 - val_loss: 0.2720\n",
      "Epoch 22/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8748 - loss: 0.3378\n",
      "Epoch 22: val_accuracy improved from 0.89953 to 0.90165, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.8748 - loss: 0.3379 - val_accuracy: 0.9017 - val_loss: 0.2650\n",
      "Epoch 23/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8791 - loss: 0.3336\n",
      "Epoch 23: val_accuracy improved from 0.90165 to 0.90377, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.8791 - loss: 0.3336 - val_accuracy: 0.9038 - val_loss: 0.2709\n",
      "Epoch 24/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8848 - loss: 0.3222\n",
      "Epoch 24: val_accuracy improved from 0.90377 to 0.91486, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - accuracy: 0.8848 - loss: 0.3222 - val_accuracy: 0.9149 - val_loss: 0.2477\n",
      "Epoch 25/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8783 - loss: 0.3346\n",
      "Epoch 25: val_accuracy did not improve from 0.91486\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.8783 - loss: 0.3346 - val_accuracy: 0.9101 - val_loss: 0.2459\n",
      "Epoch 26/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8852 - loss: 0.3257\n",
      "Epoch 26: val_accuracy did not improve from 0.91486\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.8852 - loss: 0.3257 - val_accuracy: 0.9080 - val_loss: 0.2470\n",
      "Epoch 27/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8923 - loss: 0.3059\n",
      "Epoch 27: val_accuracy improved from 0.91486 to 0.92028, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.8923 - loss: 0.3060 - val_accuracy: 0.9203 - val_loss: 0.2396\n",
      "Epoch 28/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8863 - loss: 0.3142\n",
      "Epoch 28: val_accuracy did not improve from 0.92028\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.8863 - loss: 0.3142 - val_accuracy: 0.9075 - val_loss: 0.2447\n",
      "Epoch 29/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8880 - loss: 0.3075\n",
      "Epoch 29: val_accuracy improved from 0.92028 to 0.92193, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.8879 - loss: 0.3075 - val_accuracy: 0.9219 - val_loss: 0.2259\n",
      "Epoch 30/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8909 - loss: 0.2935\n",
      "Epoch 30: val_accuracy did not improve from 0.92193\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.8909 - loss: 0.2935 - val_accuracy: 0.9156 - val_loss: 0.2302\n",
      "Epoch 31/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8881 - loss: 0.3047\n",
      "Epoch 31: val_accuracy did not improve from 0.92193\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 41ms/step - accuracy: 0.8881 - loss: 0.3047 - val_accuracy: 0.9163 - val_loss: 0.2347\n",
      "Epoch 32/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9014 - loss: 0.2796\n",
      "Epoch 32: val_accuracy did not improve from 0.92193\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 31ms/step - accuracy: 0.9014 - loss: 0.2797 - val_accuracy: 0.9193 - val_loss: 0.2252\n",
      "Epoch 33/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9000 - loss: 0.2875\n",
      "Epoch 33: val_accuracy did not improve from 0.92193\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9000 - loss: 0.2875 - val_accuracy: 0.9189 - val_loss: 0.2138\n",
      "Epoch 34/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8963 - loss: 0.2861\n",
      "Epoch 34: val_accuracy improved from 0.92193 to 0.92524, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.8963 - loss: 0.2862 - val_accuracy: 0.9252 - val_loss: 0.2161\n",
      "Epoch 35/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8943 - loss: 0.2928\n",
      "Epoch 35: val_accuracy did not improve from 0.92524\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.8944 - loss: 0.2928 - val_accuracy: 0.9226 - val_loss: 0.2192\n",
      "Epoch 36/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9053 - loss: 0.2598\n",
      "Epoch 36: val_accuracy improved from 0.92524 to 0.92547, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9053 - loss: 0.2599 - val_accuracy: 0.9255 - val_loss: 0.2142\n",
      "Epoch 37/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9062 - loss: 0.2653\n",
      "Epoch 37: val_accuracy improved from 0.92547 to 0.92783, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9061 - loss: 0.2653 - val_accuracy: 0.9278 - val_loss: 0.2115\n",
      "Epoch 38/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9042 - loss: 0.2696\n",
      "Epoch 38: val_accuracy improved from 0.92783 to 0.93420, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9042 - loss: 0.2696 - val_accuracy: 0.9342 - val_loss: 0.1938\n",
      "Epoch 39/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9070 - loss: 0.2599\n",
      "Epoch 39: val_accuracy did not improve from 0.93420\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.9070 - loss: 0.2599 - val_accuracy: 0.9295 - val_loss: 0.1917\n",
      "Epoch 40/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9037 - loss: 0.2666\n",
      "Epoch 40: val_accuracy did not improve from 0.93420\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9037 - loss: 0.2665 - val_accuracy: 0.9295 - val_loss: 0.2027\n",
      "Epoch 41/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9099 - loss: 0.2605\n",
      "Epoch 41: val_accuracy did not improve from 0.93420\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9099 - loss: 0.2605 - val_accuracy: 0.9300 - val_loss: 0.2042\n",
      "Epoch 42/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9062 - loss: 0.2564\n",
      "Epoch 42: val_accuracy did not improve from 0.93420\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9062 - loss: 0.2564 - val_accuracy: 0.9290 - val_loss: 0.2007\n",
      "Epoch 43/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9065 - loss: 0.2565\n",
      "Epoch 43: val_accuracy did not improve from 0.93420\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.9065 - loss: 0.2564 - val_accuracy: 0.9309 - val_loss: 0.1943\n",
      "Epoch 44/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9179 - loss: 0.2402\n",
      "Epoch 44: val_accuracy did not improve from 0.93420\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9179 - loss: 0.2402 - val_accuracy: 0.9283 - val_loss: 0.2096\n",
      "Epoch 45/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9105 - loss: 0.2597\n",
      "Epoch 45: val_accuracy improved from 0.93420 to 0.93797, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.9105 - loss: 0.2597 - val_accuracy: 0.9380 - val_loss: 0.1874\n",
      "Epoch 46/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9183 - loss: 0.2381\n",
      "Epoch 46: val_accuracy improved from 0.93797 to 0.93868, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9183 - loss: 0.2381 - val_accuracy: 0.9387 - val_loss: 0.1833\n",
      "Epoch 47/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9127 - loss: 0.2419\n",
      "Epoch 47: val_accuracy did not improve from 0.93868\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.9127 - loss: 0.2419 - val_accuracy: 0.9347 - val_loss: 0.1790\n",
      "Epoch 48/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9181 - loss: 0.2356\n",
      "Epoch 48: val_accuracy did not improve from 0.93868\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9181 - loss: 0.2357 - val_accuracy: 0.9342 - val_loss: 0.1864\n",
      "Epoch 49/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9236 - loss: 0.2199\n",
      "Epoch 49: val_accuracy did not improve from 0.93868\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.9235 - loss: 0.2200 - val_accuracy: 0.9335 - val_loss: 0.1841\n",
      "Epoch 50/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9161 - loss: 0.2348\n",
      "Epoch 50: val_accuracy improved from 0.93868 to 0.93892, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9161 - loss: 0.2348 - val_accuracy: 0.9389 - val_loss: 0.1808\n",
      "Epoch 51/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9191 - loss: 0.2280\n",
      "Epoch 51: val_accuracy improved from 0.93892 to 0.93915, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9191 - loss: 0.2281 - val_accuracy: 0.9392 - val_loss: 0.1687\n",
      "Epoch 52/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9243 - loss: 0.2196\n",
      "Epoch 52: val_accuracy improved from 0.93915 to 0.94269, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9243 - loss: 0.2196 - val_accuracy: 0.9427 - val_loss: 0.1698\n",
      "Epoch 53/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9215 - loss: 0.2299\n",
      "Epoch 53: val_accuracy did not improve from 0.94269\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.9215 - loss: 0.2299 - val_accuracy: 0.9413 - val_loss: 0.1688\n",
      "Epoch 54/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9213 - loss: 0.2274\n",
      "Epoch 54: val_accuracy did not improve from 0.94269\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 47ms/step - accuracy: 0.9213 - loss: 0.2274 - val_accuracy: 0.9401 - val_loss: 0.1721\n",
      "Epoch 55/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9253 - loss: 0.2101\n",
      "Epoch 55: val_accuracy did not improve from 0.94269\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.9253 - loss: 0.2102 - val_accuracy: 0.9415 - val_loss: 0.1718\n",
      "Epoch 56/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9240 - loss: 0.2192\n",
      "Epoch 56: val_accuracy improved from 0.94269 to 0.94646, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.9240 - loss: 0.2192 - val_accuracy: 0.9465 - val_loss: 0.1697\n",
      "Epoch 57/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9240 - loss: 0.2117\n",
      "Epoch 57: val_accuracy did not improve from 0.94646\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.9240 - loss: 0.2117 - val_accuracy: 0.9401 - val_loss: 0.1705\n",
      "Epoch 58/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9228 - loss: 0.2078\n",
      "Epoch 58: val_accuracy did not improve from 0.94646\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9228 - loss: 0.2078 - val_accuracy: 0.9434 - val_loss: 0.1674\n",
      "Epoch 59/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9203 - loss: 0.2228\n",
      "Epoch 59: val_accuracy did not improve from 0.94646\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 39ms/step - accuracy: 0.9204 - loss: 0.2228 - val_accuracy: 0.9422 - val_loss: 0.1657\n",
      "Epoch 60/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9227 - loss: 0.2225\n",
      "Epoch 60: val_accuracy did not improve from 0.94646\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.9227 - loss: 0.2224 - val_accuracy: 0.9422 - val_loss: 0.1640\n",
      "Epoch 61/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9250 - loss: 0.2184\n",
      "Epoch 61: val_accuracy did not improve from 0.94646\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.9250 - loss: 0.2184 - val_accuracy: 0.9399 - val_loss: 0.1571\n",
      "Epoch 62/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9298 - loss: 0.2068\n",
      "Epoch 62: val_accuracy did not improve from 0.94646\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 39ms/step - accuracy: 0.9298 - loss: 0.2068 - val_accuracy: 0.9429 - val_loss: 0.1609\n",
      "Epoch 63/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9262 - loss: 0.2099\n",
      "Epoch 63: val_accuracy did not improve from 0.94646\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 39ms/step - accuracy: 0.9262 - loss: 0.2099 - val_accuracy: 0.9394 - val_loss: 0.1652\n",
      "Epoch 64/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9222 - loss: 0.2221\n",
      "Epoch 64: val_accuracy did not improve from 0.94646\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.9222 - loss: 0.2221 - val_accuracy: 0.9446 - val_loss: 0.1565\n",
      "Epoch 65/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9319 - loss: 0.2020\n",
      "Epoch 65: val_accuracy did not improve from 0.94646\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.9319 - loss: 0.2020 - val_accuracy: 0.9462 - val_loss: 0.1510\n",
      "Epoch 66/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9309 - loss: 0.1964\n",
      "Epoch 66: val_accuracy improved from 0.94646 to 0.94811, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.9309 - loss: 0.1964 - val_accuracy: 0.9481 - val_loss: 0.1542\n",
      "Epoch 67/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9287 - loss: 0.1967\n",
      "Epoch 67: val_accuracy did not improve from 0.94811\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.9286 - loss: 0.1968 - val_accuracy: 0.9439 - val_loss: 0.1569\n",
      "Epoch 68/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9306 - loss: 0.1916\n",
      "Epoch 68: val_accuracy did not improve from 0.94811\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9306 - loss: 0.1916 - val_accuracy: 0.9455 - val_loss: 0.1599\n",
      "Epoch 69/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9306 - loss: 0.1890\n",
      "Epoch 69: val_accuracy did not improve from 0.94811\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 32ms/step - accuracy: 0.9306 - loss: 0.1891 - val_accuracy: 0.9450 - val_loss: 0.1626\n",
      "Epoch 70/300\n",
      "\u001b[1m395/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9324 - loss: 0.1953\n",
      "Epoch 70: val_accuracy did not improve from 0.94811\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9324 - loss: 0.1954 - val_accuracy: 0.9427 - val_loss: 0.1598\n",
      "Epoch 71/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9289 - loss: 0.1976\n",
      "Epoch 71: val_accuracy did not improve from 0.94811\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9289 - loss: 0.1976 - val_accuracy: 0.9406 - val_loss: 0.1640\n",
      "Epoch 72/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9301 - loss: 0.1992\n",
      "Epoch 72: val_accuracy did not improve from 0.94811\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 36ms/step - accuracy: 0.9301 - loss: 0.1992 - val_accuracy: 0.9479 - val_loss: 0.1434\n",
      "Epoch 73/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9348 - loss: 0.1911\n",
      "Epoch 73: val_accuracy improved from 0.94811 to 0.95047, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.9348 - loss: 0.1911 - val_accuracy: 0.9505 - val_loss: 0.1420\n",
      "Epoch 74/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9318 - loss: 0.1861\n",
      "Epoch 74: val_accuracy did not improve from 0.95047\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9318 - loss: 0.1862 - val_accuracy: 0.9465 - val_loss: 0.1577\n",
      "Epoch 75/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9291 - loss: 0.1967\n",
      "Epoch 75: val_accuracy did not improve from 0.95047\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 36ms/step - accuracy: 0.9291 - loss: 0.1967 - val_accuracy: 0.9481 - val_loss: 0.1537\n",
      "Epoch 76/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9343 - loss: 0.1839\n",
      "Epoch 76: val_accuracy did not improve from 0.95047\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 35ms/step - accuracy: 0.9343 - loss: 0.1839 - val_accuracy: 0.9472 - val_loss: 0.1502\n",
      "Epoch 77/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9362 - loss: 0.1823\n",
      "Epoch 77: val_accuracy did not improve from 0.95047\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.9361 - loss: 0.1823 - val_accuracy: 0.9502 - val_loss: 0.1485\n",
      "Epoch 78/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9355 - loss: 0.1794\n",
      "Epoch 78: val_accuracy did not improve from 0.95047\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9355 - loss: 0.1794 - val_accuracy: 0.9502 - val_loss: 0.1483\n",
      "Epoch 79/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9391 - loss: 0.1869\n",
      "Epoch 79: val_accuracy did not improve from 0.95047\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9391 - loss: 0.1869 - val_accuracy: 0.9500 - val_loss: 0.1451\n",
      "Epoch 80/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9288 - loss: 0.2018\n",
      "Epoch 80: val_accuracy improved from 0.95047 to 0.95236, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 37ms/step - accuracy: 0.9288 - loss: 0.2018 - val_accuracy: 0.9524 - val_loss: 0.1447\n",
      "Epoch 81/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9311 - loss: 0.1920\n",
      "Epoch 81: val_accuracy did not improve from 0.95236\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.9311 - loss: 0.1920 - val_accuracy: 0.9505 - val_loss: 0.1480\n",
      "Epoch 82/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9361 - loss: 0.1834\n",
      "Epoch 82: val_accuracy did not improve from 0.95236\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.9361 - loss: 0.1834 - val_accuracy: 0.9498 - val_loss: 0.1514\n",
      "Epoch 83/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9368 - loss: 0.1794\n",
      "Epoch 83: val_accuracy did not improve from 0.95236\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9368 - loss: 0.1795 - val_accuracy: 0.9505 - val_loss: 0.1446\n",
      "Epoch 84/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9359 - loss: 0.1813\n",
      "Epoch 84: val_accuracy did not improve from 0.95236\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9360 - loss: 0.1813 - val_accuracy: 0.9469 - val_loss: 0.1524\n",
      "Epoch 85/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9344 - loss: 0.1862\n",
      "Epoch 85: val_accuracy did not improve from 0.95236\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9344 - loss: 0.1862 - val_accuracy: 0.9505 - val_loss: 0.1494\n",
      "Epoch 86/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9366 - loss: 0.1860\n",
      "Epoch 86: val_accuracy did not improve from 0.95236\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9366 - loss: 0.1860 - val_accuracy: 0.9509 - val_loss: 0.1501\n",
      "Epoch 87/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9426 - loss: 0.1701\n",
      "Epoch 87: val_accuracy did not improve from 0.95236\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.9426 - loss: 0.1701 - val_accuracy: 0.9483 - val_loss: 0.1507\n",
      "Epoch 88/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9388 - loss: 0.1740\n",
      "Epoch 88: val_accuracy improved from 0.95236 to 0.95755, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 31ms/step - accuracy: 0.9388 - loss: 0.1740 - val_accuracy: 0.9575 - val_loss: 0.1385\n",
      "Epoch 89/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9414 - loss: 0.1726\n",
      "Epoch 89: val_accuracy did not improve from 0.95755\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 39ms/step - accuracy: 0.9414 - loss: 0.1727 - val_accuracy: 0.9547 - val_loss: 0.1365\n",
      "Epoch 90/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9413 - loss: 0.1708\n",
      "Epoch 90: val_accuracy did not improve from 0.95755\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.9413 - loss: 0.1709 - val_accuracy: 0.9559 - val_loss: 0.1348\n",
      "Epoch 91/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9366 - loss: 0.1747\n",
      "Epoch 91: val_accuracy did not improve from 0.95755\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.9366 - loss: 0.1747 - val_accuracy: 0.9559 - val_loss: 0.1344\n",
      "Epoch 92/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9437 - loss: 0.1656\n",
      "Epoch 92: val_accuracy did not improve from 0.95755\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.9437 - loss: 0.1656 - val_accuracy: 0.9512 - val_loss: 0.1434\n",
      "Epoch 93/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9374 - loss: 0.1754\n",
      "Epoch 93: val_accuracy did not improve from 0.95755\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - accuracy: 0.9375 - loss: 0.1753 - val_accuracy: 0.9526 - val_loss: 0.1408\n",
      "Epoch 94/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9404 - loss: 0.1688\n",
      "Epoch 94: val_accuracy did not improve from 0.95755\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - accuracy: 0.9404 - loss: 0.1687 - val_accuracy: 0.9547 - val_loss: 0.1376\n",
      "Epoch 95/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9447 - loss: 0.1634\n",
      "Epoch 95: val_accuracy did not improve from 0.95755\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 36ms/step - accuracy: 0.9447 - loss: 0.1634 - val_accuracy: 0.9552 - val_loss: 0.1426\n",
      "Epoch 96/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9481 - loss: 0.1532\n",
      "Epoch 96: val_accuracy did not improve from 0.95755\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 39ms/step - accuracy: 0.9481 - loss: 0.1533 - val_accuracy: 0.9502 - val_loss: 0.1477\n",
      "Epoch 97/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9433 - loss: 0.1620\n",
      "Epoch 97: val_accuracy did not improve from 0.95755\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9433 - loss: 0.1620 - val_accuracy: 0.9526 - val_loss: 0.1498\n",
      "Epoch 98/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9425 - loss: 0.1603\n",
      "Epoch 98: val_accuracy did not improve from 0.95755\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 32ms/step - accuracy: 0.9425 - loss: 0.1603 - val_accuracy: 0.9512 - val_loss: 0.1485\n",
      "Epoch 99/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9471 - loss: 0.1527\n",
      "Epoch 99: val_accuracy did not improve from 0.95755\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.9470 - loss: 0.1528 - val_accuracy: 0.9571 - val_loss: 0.1372\n",
      "Epoch 100/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9468 - loss: 0.1602\n",
      "Epoch 100: val_accuracy improved from 0.95755 to 0.95920, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9468 - loss: 0.1602 - val_accuracy: 0.9592 - val_loss: 0.1318\n",
      "Epoch 101/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9476 - loss: 0.1488\n",
      "Epoch 101: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9476 - loss: 0.1489 - val_accuracy: 0.9535 - val_loss: 0.1354\n",
      "Epoch 102/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9466 - loss: 0.1511\n",
      "Epoch 102: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9466 - loss: 0.1512 - val_accuracy: 0.9533 - val_loss: 0.1361\n",
      "Epoch 103/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9462 - loss: 0.1551\n",
      "Epoch 103: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9462 - loss: 0.1551 - val_accuracy: 0.9557 - val_loss: 0.1367\n",
      "Epoch 104/300\n",
      "\u001b[1m395/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9445 - loss: 0.1639\n",
      "Epoch 104: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9445 - loss: 0.1639 - val_accuracy: 0.9540 - val_loss: 0.1328\n",
      "Epoch 105/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9458 - loss: 0.1524\n",
      "Epoch 105: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9458 - loss: 0.1524 - val_accuracy: 0.9540 - val_loss: 0.1357\n",
      "Epoch 106/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9531 - loss: 0.1329\n",
      "Epoch 106: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9531 - loss: 0.1330 - val_accuracy: 0.9557 - val_loss: 0.1250\n",
      "Epoch 107/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9466 - loss: 0.1631\n",
      "Epoch 107: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9466 - loss: 0.1631 - val_accuracy: 0.9571 - val_loss: 0.1248\n",
      "Epoch 108/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9537 - loss: 0.1363\n",
      "Epoch 108: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9536 - loss: 0.1363 - val_accuracy: 0.9580 - val_loss: 0.1261\n",
      "Epoch 109/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9490 - loss: 0.1444\n",
      "Epoch 109: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.9490 - loss: 0.1444 - val_accuracy: 0.9578 - val_loss: 0.1276\n",
      "Epoch 110/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9499 - loss: 0.1511\n",
      "Epoch 110: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.9499 - loss: 0.1511 - val_accuracy: 0.9554 - val_loss: 0.1285\n",
      "Epoch 111/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9469 - loss: 0.1574\n",
      "Epoch 111: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9469 - loss: 0.1574 - val_accuracy: 0.9575 - val_loss: 0.1253\n",
      "Epoch 112/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9490 - loss: 0.1460\n",
      "Epoch 112: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9490 - loss: 0.1460 - val_accuracy: 0.9568 - val_loss: 0.1276\n",
      "Epoch 113/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9483 - loss: 0.1541\n",
      "Epoch 113: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9482 - loss: 0.1541 - val_accuracy: 0.9585 - val_loss: 0.1291\n",
      "Epoch 114/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9479 - loss: 0.1450\n",
      "Epoch 114: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9479 - loss: 0.1450 - val_accuracy: 0.9592 - val_loss: 0.1269\n",
      "Epoch 115/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9475 - loss: 0.1584\n",
      "Epoch 115: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.9475 - loss: 0.1584 - val_accuracy: 0.9578 - val_loss: 0.1300\n",
      "Epoch 116/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9476 - loss: 0.1494\n",
      "Epoch 116: val_accuracy improved from 0.95920 to 0.95967, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9476 - loss: 0.1494 - val_accuracy: 0.9597 - val_loss: 0.1258\n",
      "Epoch 117/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9461 - loss: 0.1578\n",
      "Epoch 117: val_accuracy did not improve from 0.95967\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9461 - loss: 0.1578 - val_accuracy: 0.9568 - val_loss: 0.1254\n",
      "Epoch 118/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9502 - loss: 0.1487\n",
      "Epoch 118: val_accuracy did not improve from 0.95967\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9502 - loss: 0.1487 - val_accuracy: 0.9583 - val_loss: 0.1298\n",
      "Epoch 119/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9492 - loss: 0.1446\n",
      "Epoch 119: val_accuracy did not improve from 0.95967\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9492 - loss: 0.1445 - val_accuracy: 0.9597 - val_loss: 0.1252\n",
      "Epoch 120/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9540 - loss: 0.1392\n",
      "Epoch 120: val_accuracy improved from 0.95967 to 0.96344, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9539 - loss: 0.1393 - val_accuracy: 0.9634 - val_loss: 0.1169\n",
      "Epoch 121/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9493 - loss: 0.1445\n",
      "Epoch 121: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.9493 - loss: 0.1445 - val_accuracy: 0.9630 - val_loss: 0.1197\n",
      "Epoch 122/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9508 - loss: 0.1433\n",
      "Epoch 122: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9508 - loss: 0.1433 - val_accuracy: 0.9630 - val_loss: 0.1236\n",
      "Epoch 123/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9536 - loss: 0.1330\n",
      "Epoch 123: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9536 - loss: 0.1330 - val_accuracy: 0.9606 - val_loss: 0.1197\n",
      "Epoch 124/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9518 - loss: 0.1351\n",
      "Epoch 124: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9518 - loss: 0.1351 - val_accuracy: 0.9585 - val_loss: 0.1299\n",
      "Epoch 125/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9488 - loss: 0.1376\n",
      "Epoch 125: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9488 - loss: 0.1376 - val_accuracy: 0.9580 - val_loss: 0.1322\n",
      "Epoch 126/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9502 - loss: 0.1392\n",
      "Epoch 126: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9502 - loss: 0.1392 - val_accuracy: 0.9599 - val_loss: 0.1227\n",
      "Epoch 127/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9546 - loss: 0.1386\n",
      "Epoch 127: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.9546 - loss: 0.1386 - val_accuracy: 0.9613 - val_loss: 0.1226\n",
      "Epoch 128/300\n",
      "\u001b[1m395/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9522 - loss: 0.1368\n",
      "Epoch 128: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9522 - loss: 0.1368 - val_accuracy: 0.9573 - val_loss: 0.1288\n",
      "Epoch 129/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9485 - loss: 0.1479\n",
      "Epoch 129: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.9485 - loss: 0.1479 - val_accuracy: 0.9590 - val_loss: 0.1259\n",
      "Epoch 130/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9488 - loss: 0.1471\n",
      "Epoch 130: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 32ms/step - accuracy: 0.9488 - loss: 0.1470 - val_accuracy: 0.9597 - val_loss: 0.1233\n",
      "Epoch 131/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9536 - loss: 0.1366\n",
      "Epoch 131: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9536 - loss: 0.1366 - val_accuracy: 0.9599 - val_loss: 0.1223\n",
      "Epoch 132/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9498 - loss: 0.1506\n",
      "Epoch 132: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9498 - loss: 0.1506 - val_accuracy: 0.9550 - val_loss: 0.1351\n",
      "Epoch 133/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9531 - loss: 0.1397\n",
      "Epoch 133: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9531 - loss: 0.1397 - val_accuracy: 0.9573 - val_loss: 0.1288\n",
      "Epoch 134/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9525 - loss: 0.1330\n",
      "Epoch 134: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9525 - loss: 0.1330 - val_accuracy: 0.9592 - val_loss: 0.1215\n",
      "Epoch 135/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9497 - loss: 0.1393\n",
      "Epoch 135: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9497 - loss: 0.1393 - val_accuracy: 0.9611 - val_loss: 0.1305\n",
      "Epoch 136/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9552 - loss: 0.1314\n",
      "Epoch 136: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.9552 - loss: 0.1314 - val_accuracy: 0.9592 - val_loss: 0.1186\n",
      "Epoch 137/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9527 - loss: 0.1315\n",
      "Epoch 137: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9527 - loss: 0.1315 - val_accuracy: 0.9611 - val_loss: 0.1211\n",
      "Epoch 138/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9526 - loss: 0.1373\n",
      "Epoch 138: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9526 - loss: 0.1373 - val_accuracy: 0.9625 - val_loss: 0.1272\n",
      "Epoch 139/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9562 - loss: 0.1360\n",
      "Epoch 139: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9562 - loss: 0.1360 - val_accuracy: 0.9564 - val_loss: 0.1308\n",
      "Epoch 140/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9557 - loss: 0.1269\n",
      "Epoch 140: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9557 - loss: 0.1269 - val_accuracy: 0.9587 - val_loss: 0.1304\n",
      "Epoch 141/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9562 - loss: 0.1334\n",
      "Epoch 141: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9562 - loss: 0.1334 - val_accuracy: 0.9611 - val_loss: 0.1246\n",
      "Epoch 142/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9557 - loss: 0.1317\n",
      "Epoch 142: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9557 - loss: 0.1317 - val_accuracy: 0.9618 - val_loss: 0.1227\n",
      "Epoch 143/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9528 - loss: 0.1394\n",
      "Epoch 143: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9528 - loss: 0.1394 - val_accuracy: 0.9604 - val_loss: 0.1168\n",
      "Epoch 144/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9521 - loss: 0.1362\n",
      "Epoch 144: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9521 - loss: 0.1362 - val_accuracy: 0.9601 - val_loss: 0.1197\n",
      "Epoch 145/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9564 - loss: 0.1260\n",
      "Epoch 145: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.9564 - loss: 0.1260 - val_accuracy: 0.9620 - val_loss: 0.1209\n",
      "Epoch 146/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9533 - loss: 0.1364\n",
      "Epoch 146: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9533 - loss: 0.1364 - val_accuracy: 0.9592 - val_loss: 0.1157\n",
      "Epoch 147/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9554 - loss: 0.1243\n",
      "Epoch 147: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9554 - loss: 0.1244 - val_accuracy: 0.9613 - val_loss: 0.1164\n",
      "Epoch 148/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9553 - loss: 0.1253\n",
      "Epoch 148: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9553 - loss: 0.1254 - val_accuracy: 0.9606 - val_loss: 0.1186\n",
      "Epoch 149/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9576 - loss: 0.1189\n",
      "Epoch 149: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9576 - loss: 0.1190 - val_accuracy: 0.9608 - val_loss: 0.1199\n",
      "Epoch 150/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9568 - loss: 0.1269\n",
      "Epoch 150: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9568 - loss: 0.1270 - val_accuracy: 0.9592 - val_loss: 0.1213\n",
      "Epoch 151/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9580 - loss: 0.1221\n",
      "Epoch 151: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9580 - loss: 0.1221 - val_accuracy: 0.9597 - val_loss: 0.1187\n",
      "Epoch 152/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9581 - loss: 0.1267\n",
      "Epoch 152: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9581 - loss: 0.1268 - val_accuracy: 0.9616 - val_loss: 0.1138\n",
      "Epoch 153/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9528 - loss: 0.1305\n",
      "Epoch 153: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.9528 - loss: 0.1305 - val_accuracy: 0.9606 - val_loss: 0.1123\n",
      "Epoch 154/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9586 - loss: 0.1202\n",
      "Epoch 154: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9586 - loss: 0.1202 - val_accuracy: 0.9573 - val_loss: 0.1265\n",
      "Epoch 155/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9579 - loss: 0.1171\n",
      "Epoch 155: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9579 - loss: 0.1171 - val_accuracy: 0.9540 - val_loss: 0.1323\n",
      "Epoch 156/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9572 - loss: 0.1242\n",
      "Epoch 156: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9572 - loss: 0.1242 - val_accuracy: 0.9592 - val_loss: 0.1240\n",
      "Epoch 157/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9592 - loss: 0.1190\n",
      "Epoch 157: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9592 - loss: 0.1191 - val_accuracy: 0.9608 - val_loss: 0.1279\n",
      "Epoch 158/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9606 - loss: 0.1172\n",
      "Epoch 158: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9606 - loss: 0.1172 - val_accuracy: 0.9594 - val_loss: 0.1293\n",
      "Epoch 159/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9620 - loss: 0.1151\n",
      "Epoch 159: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9620 - loss: 0.1152 - val_accuracy: 0.9599 - val_loss: 0.1254\n",
      "Epoch 160/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9593 - loss: 0.1289\n",
      "Epoch 160: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9593 - loss: 0.1289 - val_accuracy: 0.9592 - val_loss: 0.1217\n",
      "Epoch 161/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9524 - loss: 0.1500\n",
      "Epoch 161: val_accuracy improved from 0.96344 to 0.96439, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9524 - loss: 0.1499 - val_accuracy: 0.9644 - val_loss: 0.1121\n",
      "Epoch 162/300\n",
      "\u001b[1m395/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9546 - loss: 0.1282\n",
      "Epoch 162: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.9546 - loss: 0.1282 - val_accuracy: 0.9632 - val_loss: 0.1143\n",
      "Epoch 163/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9550 - loss: 0.1330\n",
      "Epoch 163: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9550 - loss: 0.1330 - val_accuracy: 0.9594 - val_loss: 0.1165\n",
      "Epoch 164/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9591 - loss: 0.1205\n",
      "Epoch 164: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9591 - loss: 0.1205 - val_accuracy: 0.9601 - val_loss: 0.1216\n",
      "Epoch 165/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9604 - loss: 0.1246\n",
      "Epoch 165: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9604 - loss: 0.1246 - val_accuracy: 0.9625 - val_loss: 0.1175\n",
      "Epoch 166/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9638 - loss: 0.1112\n",
      "Epoch 166: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9638 - loss: 0.1112 - val_accuracy: 0.9620 - val_loss: 0.1139\n",
      "Epoch 167/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9592 - loss: 0.1149\n",
      "Epoch 167: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9592 - loss: 0.1149 - val_accuracy: 0.9639 - val_loss: 0.1136\n",
      "Epoch 168/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9636 - loss: 0.1048\n",
      "Epoch 168: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9636 - loss: 0.1048 - val_accuracy: 0.9620 - val_loss: 0.1189\n",
      "Epoch 169/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9564 - loss: 0.1257\n",
      "Epoch 169: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9564 - loss: 0.1257 - val_accuracy: 0.9616 - val_loss: 0.1163\n",
      "Epoch 170/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9577 - loss: 0.1235\n",
      "Epoch 170: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.9577 - loss: 0.1235 - val_accuracy: 0.9627 - val_loss: 0.1161\n",
      "Epoch 171/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9542 - loss: 0.1301\n",
      "Epoch 171: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9542 - loss: 0.1301 - val_accuracy: 0.9634 - val_loss: 0.1116\n",
      "Epoch 172/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9560 - loss: 0.1222\n",
      "Epoch 172: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9560 - loss: 0.1222 - val_accuracy: 0.9606 - val_loss: 0.1183\n",
      "Epoch 173/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9594 - loss: 0.1149\n",
      "Epoch 173: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9594 - loss: 0.1149 - val_accuracy: 0.9627 - val_loss: 0.1177\n",
      "Epoch 174/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9581 - loss: 0.1187\n",
      "Epoch 174: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9581 - loss: 0.1187 - val_accuracy: 0.9639 - val_loss: 0.1116\n",
      "Epoch 175/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9629 - loss: 0.1105\n",
      "Epoch 175: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9629 - loss: 0.1105 - val_accuracy: 0.9642 - val_loss: 0.1116\n",
      "Epoch 176/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9623 - loss: 0.1133\n",
      "Epoch 176: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.9623 - loss: 0.1134 - val_accuracy: 0.9625 - val_loss: 0.1191\n",
      "Epoch 177/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9621 - loss: 0.1105\n",
      "Epoch 177: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9621 - loss: 0.1105 - val_accuracy: 0.9623 - val_loss: 0.1224\n",
      "Epoch 178/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9596 - loss: 0.1250\n",
      "Epoch 178: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9596 - loss: 0.1250 - val_accuracy: 0.9620 - val_loss: 0.1165\n",
      "Epoch 179/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9670 - loss: 0.1033\n",
      "Epoch 179: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.9670 - loss: 0.1034 - val_accuracy: 0.9625 - val_loss: 0.1188\n",
      "Epoch 180/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9621 - loss: 0.1163\n",
      "Epoch 180: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9621 - loss: 0.1163 - val_accuracy: 0.9608 - val_loss: 0.1207\n",
      "Epoch 181/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9613 - loss: 0.1158\n",
      "Epoch 181: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9613 - loss: 0.1158 - val_accuracy: 0.9616 - val_loss: 0.1140\n",
      "Epoch 182/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9628 - loss: 0.1173\n",
      "Epoch 182: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9628 - loss: 0.1173 - val_accuracy: 0.9634 - val_loss: 0.1187\n",
      "Epoch 183/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9578 - loss: 0.1246\n",
      "Epoch 183: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9578 - loss: 0.1246 - val_accuracy: 0.9642 - val_loss: 0.1128\n",
      "Epoch 184/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9588 - loss: 0.1246\n",
      "Epoch 184: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9588 - loss: 0.1246 - val_accuracy: 0.9644 - val_loss: 0.1094\n",
      "Epoch 185/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9613 - loss: 0.1232\n",
      "Epoch 185: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9613 - loss: 0.1231 - val_accuracy: 0.9620 - val_loss: 0.1158\n",
      "Epoch 186/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9586 - loss: 0.1213\n",
      "Epoch 186: val_accuracy did not improve from 0.96439\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9586 - loss: 0.1213 - val_accuracy: 0.9611 - val_loss: 0.1214\n",
      "Epoch 187/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9588 - loss: 0.1189\n",
      "Epoch 187: val_accuracy improved from 0.96439 to 0.96533, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9588 - loss: 0.1189 - val_accuracy: 0.9653 - val_loss: 0.1131\n",
      "Epoch 188/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9572 - loss: 0.1250\n",
      "Epoch 188: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.9572 - loss: 0.1249 - val_accuracy: 0.9651 - val_loss: 0.1136\n",
      "Epoch 189/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9595 - loss: 0.1135\n",
      "Epoch 189: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9595 - loss: 0.1136 - val_accuracy: 0.9625 - val_loss: 0.1158\n",
      "Epoch 190/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9598 - loss: 0.1160\n",
      "Epoch 190: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9598 - loss: 0.1160 - val_accuracy: 0.9649 - val_loss: 0.1108\n",
      "Epoch 191/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9618 - loss: 0.1090\n",
      "Epoch 191: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9618 - loss: 0.1090 - val_accuracy: 0.9630 - val_loss: 0.1210\n",
      "Epoch 192/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9612 - loss: 0.1087\n",
      "Epoch 192: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9612 - loss: 0.1087 - val_accuracy: 0.9597 - val_loss: 0.1195\n",
      "Epoch 193/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9581 - loss: 0.1247\n",
      "Epoch 193: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9581 - loss: 0.1247 - val_accuracy: 0.9644 - val_loss: 0.1082\n",
      "Epoch 194/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9634 - loss: 0.1050\n",
      "Epoch 194: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9634 - loss: 0.1050 - val_accuracy: 0.9599 - val_loss: 0.1170\n",
      "Epoch 195/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9633 - loss: 0.1084\n",
      "Epoch 195: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9633 - loss: 0.1085 - val_accuracy: 0.9646 - val_loss: 0.1109\n",
      "Epoch 196/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9624 - loss: 0.1143\n",
      "Epoch 196: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9624 - loss: 0.1144 - val_accuracy: 0.9644 - val_loss: 0.1165\n",
      "Epoch 197/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9654 - loss: 0.1036\n",
      "Epoch 197: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9654 - loss: 0.1036 - val_accuracy: 0.9632 - val_loss: 0.1164\n",
      "Epoch 198/300\n",
      "\u001b[1m395/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9641 - loss: 0.1027\n",
      "Epoch 198: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.9641 - loss: 0.1027 - val_accuracy: 0.9630 - val_loss: 0.1161\n",
      "Epoch 199/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9622 - loss: 0.1180\n",
      "Epoch 199: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.9622 - loss: 0.1181 - val_accuracy: 0.9613 - val_loss: 0.1188\n",
      "Epoch 200/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9619 - loss: 0.1081\n",
      "Epoch 200: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9619 - loss: 0.1081 - val_accuracy: 0.9597 - val_loss: 0.1200\n",
      "Epoch 201/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9629 - loss: 0.1029\n",
      "Epoch 201: val_accuracy did not improve from 0.96533\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9629 - loss: 0.1029 - val_accuracy: 0.9639 - val_loss: 0.1121\n",
      "Epoch 202/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9692 - loss: 0.0937\n",
      "Epoch 202: val_accuracy improved from 0.96533 to 0.96604, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9692 - loss: 0.0938 - val_accuracy: 0.9660 - val_loss: 0.1101\n",
      "Epoch 203/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9653 - loss: 0.1070\n",
      "Epoch 203: val_accuracy did not improve from 0.96604\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9653 - loss: 0.1070 - val_accuracy: 0.9656 - val_loss: 0.1157\n",
      "Epoch 204/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9627 - loss: 0.1051\n",
      "Epoch 204: val_accuracy did not improve from 0.96604\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9627 - loss: 0.1052 - val_accuracy: 0.9642 - val_loss: 0.1165\n",
      "Epoch 205/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9551 - loss: 0.1250\n",
      "Epoch 205: val_accuracy did not improve from 0.96604\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9551 - loss: 0.1250 - val_accuracy: 0.9637 - val_loss: 0.1184\n",
      "Epoch 206/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9650 - loss: 0.0995\n",
      "Epoch 206: val_accuracy did not improve from 0.96604\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9650 - loss: 0.0995 - val_accuracy: 0.9634 - val_loss: 0.1176\n",
      "Epoch 207/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9638 - loss: 0.1061\n",
      "Epoch 207: val_accuracy did not improve from 0.96604\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9638 - loss: 0.1062 - val_accuracy: 0.9658 - val_loss: 0.1064\n",
      "Epoch 208/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9638 - loss: 0.1049\n",
      "Epoch 208: val_accuracy did not improve from 0.96604\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9638 - loss: 0.1049 - val_accuracy: 0.9627 - val_loss: 0.1188\n",
      "Epoch 209/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9636 - loss: 0.1086\n",
      "Epoch 209: val_accuracy did not improve from 0.96604\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9636 - loss: 0.1086 - val_accuracy: 0.9646 - val_loss: 0.1081\n",
      "Epoch 210/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9653 - loss: 0.1094\n",
      "Epoch 210: val_accuracy did not improve from 0.96604\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.9653 - loss: 0.1094 - val_accuracy: 0.9639 - val_loss: 0.1131\n",
      "Epoch 211/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9621 - loss: 0.1108\n",
      "Epoch 211: val_accuracy did not improve from 0.96604\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9621 - loss: 0.1108 - val_accuracy: 0.9623 - val_loss: 0.1217\n",
      "Epoch 212/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9660 - loss: 0.1115\n",
      "Epoch 212: val_accuracy did not improve from 0.96604\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9660 - loss: 0.1115 - val_accuracy: 0.9651 - val_loss: 0.1160\n",
      "Epoch 213/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9637 - loss: 0.1059\n",
      "Epoch 213: val_accuracy did not improve from 0.96604\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9637 - loss: 0.1059 - val_accuracy: 0.9627 - val_loss: 0.1208\n",
      "Epoch 214/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9643 - loss: 0.0999\n",
      "Epoch 214: val_accuracy did not improve from 0.96604\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9643 - loss: 0.0999 - val_accuracy: 0.9606 - val_loss: 0.1269\n",
      "Epoch 215/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9629 - loss: 0.1062\n",
      "Epoch 215: val_accuracy improved from 0.96604 to 0.96675, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9628 - loss: 0.1062 - val_accuracy: 0.9667 - val_loss: 0.1173\n",
      "Epoch 216/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9651 - loss: 0.1020\n",
      "Epoch 216: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9651 - loss: 0.1021 - val_accuracy: 0.9625 - val_loss: 0.1169\n",
      "Epoch 217/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9612 - loss: 0.1175\n",
      "Epoch 217: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9612 - loss: 0.1175 - val_accuracy: 0.9639 - val_loss: 0.1205\n",
      "Epoch 218/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9652 - loss: 0.1053\n",
      "Epoch 218: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9652 - loss: 0.1053 - val_accuracy: 0.9660 - val_loss: 0.1177\n",
      "Epoch 219/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9584 - loss: 0.1220\n",
      "Epoch 219: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9584 - loss: 0.1220 - val_accuracy: 0.9658 - val_loss: 0.1118\n",
      "Epoch 220/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9641 - loss: 0.1077\n",
      "Epoch 220: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.9641 - loss: 0.1077 - val_accuracy: 0.9618 - val_loss: 0.1160\n",
      "Epoch 221/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9657 - loss: 0.0973\n",
      "Epoch 221: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9657 - loss: 0.0973 - val_accuracy: 0.9646 - val_loss: 0.1076\n",
      "Epoch 222/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9654 - loss: 0.1002\n",
      "Epoch 222: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9654 - loss: 0.1002 - val_accuracy: 0.9644 - val_loss: 0.1195\n",
      "Epoch 223/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9660 - loss: 0.1053\n",
      "Epoch 223: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9660 - loss: 0.1053 - val_accuracy: 0.9623 - val_loss: 0.1193\n",
      "Epoch 224/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9605 - loss: 0.1162\n",
      "Epoch 224: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9605 - loss: 0.1161 - val_accuracy: 0.9608 - val_loss: 0.1209\n",
      "Epoch 225/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9603 - loss: 0.1163\n",
      "Epoch 225: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.9603 - loss: 0.1163 - val_accuracy: 0.9637 - val_loss: 0.1190\n",
      "Epoch 226/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9619 - loss: 0.1079\n",
      "Epoch 226: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9619 - loss: 0.1079 - val_accuracy: 0.9634 - val_loss: 0.1215\n",
      "Epoch 227/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9664 - loss: 0.1001\n",
      "Epoch 227: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.9664 - loss: 0.1001 - val_accuracy: 0.9630 - val_loss: 0.1211\n",
      "Epoch 228/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9641 - loss: 0.1100\n",
      "Epoch 228: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9641 - loss: 0.1100 - val_accuracy: 0.9639 - val_loss: 0.1146\n",
      "Epoch 229/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9634 - loss: 0.1034\n",
      "Epoch 229: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9634 - loss: 0.1034 - val_accuracy: 0.9623 - val_loss: 0.1131\n",
      "Epoch 230/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9628 - loss: 0.1061\n",
      "Epoch 230: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9628 - loss: 0.1061 - val_accuracy: 0.9623 - val_loss: 0.1171\n",
      "Epoch 231/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9656 - loss: 0.1090\n",
      "Epoch 231: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9656 - loss: 0.1090 - val_accuracy: 0.9611 - val_loss: 0.1120\n",
      "Epoch 232/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9677 - loss: 0.1042\n",
      "Epoch 232: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9677 - loss: 0.1042 - val_accuracy: 0.9632 - val_loss: 0.1166\n",
      "Epoch 233/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9646 - loss: 0.1080\n",
      "Epoch 233: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9646 - loss: 0.1080 - val_accuracy: 0.9656 - val_loss: 0.1110\n",
      "Epoch 234/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9668 - loss: 0.1035\n",
      "Epoch 234: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9668 - loss: 0.1035 - val_accuracy: 0.9642 - val_loss: 0.1089\n",
      "Epoch 235/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9662 - loss: 0.0975\n",
      "Epoch 235: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9662 - loss: 0.0975 - val_accuracy: 0.9649 - val_loss: 0.1124\n",
      "Epoch 236/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9690 - loss: 0.1028\n",
      "Epoch 236: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9690 - loss: 0.1028 - val_accuracy: 0.9644 - val_loss: 0.1158\n",
      "Epoch 237/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9677 - loss: 0.0958\n",
      "Epoch 237: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9677 - loss: 0.0959 - val_accuracy: 0.9625 - val_loss: 0.1117\n",
      "Epoch 238/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9656 - loss: 0.0957\n",
      "Epoch 238: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9656 - loss: 0.0957 - val_accuracy: 0.9613 - val_loss: 0.1200\n",
      "Epoch 239/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9635 - loss: 0.1093\n",
      "Epoch 239: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9635 - loss: 0.1093 - val_accuracy: 0.9649 - val_loss: 0.1115\n",
      "Epoch 240/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9648 - loss: 0.1037\n",
      "Epoch 240: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9648 - loss: 0.1037 - val_accuracy: 0.9660 - val_loss: 0.1105\n",
      "Epoch 241/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9687 - loss: 0.0956\n",
      "Epoch 241: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9687 - loss: 0.0956 - val_accuracy: 0.9667 - val_loss: 0.1126\n",
      "Epoch 242/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9647 - loss: 0.1017\n",
      "Epoch 242: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9647 - loss: 0.1016 - val_accuracy: 0.9660 - val_loss: 0.1155\n",
      "Epoch 243/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9638 - loss: 0.1057\n",
      "Epoch 243: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9638 - loss: 0.1057 - val_accuracy: 0.9649 - val_loss: 0.1120\n",
      "Epoch 244/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9701 - loss: 0.0871\n",
      "Epoch 244: val_accuracy did not improve from 0.96675\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9701 - loss: 0.0871 - val_accuracy: 0.9658 - val_loss: 0.1145\n",
      "Epoch 245/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9675 - loss: 0.0910\n",
      "Epoch 245: val_accuracy improved from 0.96675 to 0.96698, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9675 - loss: 0.0911 - val_accuracy: 0.9670 - val_loss: 0.1157\n",
      "Epoch 246/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9666 - loss: 0.1005\n",
      "Epoch 246: val_accuracy did not improve from 0.96698\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9666 - loss: 0.1005 - val_accuracy: 0.9623 - val_loss: 0.1138\n",
      "Epoch 247/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9714 - loss: 0.0907\n",
      "Epoch 247: val_accuracy did not improve from 0.96698\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9713 - loss: 0.0907 - val_accuracy: 0.9665 - val_loss: 0.1170\n",
      "Epoch 248/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9665 - loss: 0.0996\n",
      "Epoch 248: val_accuracy did not improve from 0.96698\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9665 - loss: 0.0997 - val_accuracy: 0.9653 - val_loss: 0.1163\n",
      "Epoch 249/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9677 - loss: 0.0953\n",
      "Epoch 249: val_accuracy did not improve from 0.96698\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9677 - loss: 0.0953 - val_accuracy: 0.9642 - val_loss: 0.1166\n",
      "Epoch 250/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9644 - loss: 0.1016\n",
      "Epoch 250: val_accuracy did not improve from 0.96698\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9644 - loss: 0.1016 - val_accuracy: 0.9623 - val_loss: 0.1128\n",
      "Epoch 251/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9719 - loss: 0.0916\n",
      "Epoch 251: val_accuracy did not improve from 0.96698\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9719 - loss: 0.0916 - val_accuracy: 0.9644 - val_loss: 0.1123\n",
      "Epoch 252/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9693 - loss: 0.0935\n",
      "Epoch 252: val_accuracy improved from 0.96698 to 0.96745, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9693 - loss: 0.0935 - val_accuracy: 0.9675 - val_loss: 0.1105\n",
      "Epoch 253/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9641 - loss: 0.1119\n",
      "Epoch 253: val_accuracy did not improve from 0.96745\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9641 - loss: 0.1119 - val_accuracy: 0.9623 - val_loss: 0.1121\n",
      "Epoch 254/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9667 - loss: 0.0950\n",
      "Epoch 254: val_accuracy did not improve from 0.96745\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9667 - loss: 0.0950 - val_accuracy: 0.9663 - val_loss: 0.1051\n",
      "Epoch 255/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9662 - loss: 0.1015\n",
      "Epoch 255: val_accuracy did not improve from 0.96745\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9662 - loss: 0.1015 - val_accuracy: 0.9653 - val_loss: 0.1059\n",
      "Epoch 256/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9674 - loss: 0.0975\n",
      "Epoch 256: val_accuracy improved from 0.96745 to 0.96792, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9674 - loss: 0.0975 - val_accuracy: 0.9679 - val_loss: 0.1087\n",
      "Epoch 257/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9665 - loss: 0.0997\n",
      "Epoch 257: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9665 - loss: 0.0997 - val_accuracy: 0.9670 - val_loss: 0.1089\n",
      "Epoch 258/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9654 - loss: 0.0993\n",
      "Epoch 258: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9654 - loss: 0.0993 - val_accuracy: 0.9642 - val_loss: 0.1141\n",
      "Epoch 259/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9708 - loss: 0.0879\n",
      "Epoch 259: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9708 - loss: 0.0879 - val_accuracy: 0.9660 - val_loss: 0.1128\n",
      "Epoch 260/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9676 - loss: 0.0933\n",
      "Epoch 260: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.9676 - loss: 0.0933 - val_accuracy: 0.9642 - val_loss: 0.1147\n",
      "Epoch 261/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9665 - loss: 0.1070\n",
      "Epoch 261: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9665 - loss: 0.1070 - val_accuracy: 0.9658 - val_loss: 0.1125\n",
      "Epoch 262/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9696 - loss: 0.0940\n",
      "Epoch 262: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9696 - loss: 0.0941 - val_accuracy: 0.9658 - val_loss: 0.1136\n",
      "Epoch 263/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9662 - loss: 0.1029\n",
      "Epoch 263: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9662 - loss: 0.1029 - val_accuracy: 0.9658 - val_loss: 0.1067\n",
      "Epoch 264/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9709 - loss: 0.0826\n",
      "Epoch 264: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9709 - loss: 0.0826 - val_accuracy: 0.9632 - val_loss: 0.1189\n",
      "Epoch 265/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9665 - loss: 0.1006\n",
      "Epoch 265: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9665 - loss: 0.1006 - val_accuracy: 0.9642 - val_loss: 0.1115\n",
      "Epoch 266/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9692 - loss: 0.0889\n",
      "Epoch 266: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9692 - loss: 0.0889 - val_accuracy: 0.9646 - val_loss: 0.1126\n",
      "Epoch 267/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9648 - loss: 0.1014\n",
      "Epoch 267: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9648 - loss: 0.1013 - val_accuracy: 0.9646 - val_loss: 0.1158\n",
      "Epoch 268/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9681 - loss: 0.0961\n",
      "Epoch 268: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9681 - loss: 0.0961 - val_accuracy: 0.9667 - val_loss: 0.1117\n",
      "Epoch 269/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9701 - loss: 0.0875\n",
      "Epoch 269: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9701 - loss: 0.0875 - val_accuracy: 0.9653 - val_loss: 0.1146\n",
      "Epoch 270/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9676 - loss: 0.0957\n",
      "Epoch 270: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9676 - loss: 0.0958 - val_accuracy: 0.9623 - val_loss: 0.1139\n",
      "Epoch 271/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9687 - loss: 0.0950\n",
      "Epoch 271: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9686 - loss: 0.0950 - val_accuracy: 0.9627 - val_loss: 0.1207\n",
      "Epoch 272/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9684 - loss: 0.0953\n",
      "Epoch 272: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 32ms/step - accuracy: 0.9684 - loss: 0.0953 - val_accuracy: 0.9660 - val_loss: 0.1188\n",
      "Epoch 273/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9709 - loss: 0.0949\n",
      "Epoch 273: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9709 - loss: 0.0949 - val_accuracy: 0.9646 - val_loss: 0.1146\n",
      "Epoch 274/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9666 - loss: 0.1083\n",
      "Epoch 274: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.9666 - loss: 0.1083 - val_accuracy: 0.9660 - val_loss: 0.1113\n",
      "Epoch 275/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9708 - loss: 0.0833\n",
      "Epoch 275: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9708 - loss: 0.0833 - val_accuracy: 0.9646 - val_loss: 0.1251\n",
      "Epoch 276/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9673 - loss: 0.1008\n",
      "Epoch 276: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9673 - loss: 0.1008 - val_accuracy: 0.9663 - val_loss: 0.1141\n",
      "Epoch 277/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9699 - loss: 0.0882\n",
      "Epoch 277: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9699 - loss: 0.0882 - val_accuracy: 0.9630 - val_loss: 0.1167\n",
      "Epoch 278/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9688 - loss: 0.0918\n",
      "Epoch 278: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9688 - loss: 0.0918 - val_accuracy: 0.9672 - val_loss: 0.1191\n",
      "Epoch 279/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9650 - loss: 0.0966\n",
      "Epoch 279: val_accuracy did not improve from 0.96792\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9650 - loss: 0.0966 - val_accuracy: 0.9670 - val_loss: 0.1107\n",
      "Epoch 280/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9703 - loss: 0.0931\n",
      "Epoch 280: val_accuracy improved from 0.96792 to 0.96887, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9703 - loss: 0.0931 - val_accuracy: 0.9689 - val_loss: 0.1040\n",
      "Epoch 281/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9669 - loss: 0.0937\n",
      "Epoch 281: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9670 - loss: 0.0937 - val_accuracy: 0.9656 - val_loss: 0.1094\n",
      "Epoch 282/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9715 - loss: 0.0883\n",
      "Epoch 282: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9715 - loss: 0.0883 - val_accuracy: 0.9679 - val_loss: 0.1083\n",
      "Epoch 283/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9670 - loss: 0.1003\n",
      "Epoch 283: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9670 - loss: 0.1003 - val_accuracy: 0.9646 - val_loss: 0.1158\n",
      "Epoch 284/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9711 - loss: 0.0792\n",
      "Epoch 284: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9711 - loss: 0.0792 - val_accuracy: 0.9649 - val_loss: 0.1176\n",
      "Epoch 285/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9666 - loss: 0.0952\n",
      "Epoch 285: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9666 - loss: 0.0952 - val_accuracy: 0.9639 - val_loss: 0.1120\n",
      "Epoch 286/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9713 - loss: 0.0880\n",
      "Epoch 286: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9713 - loss: 0.0880 - val_accuracy: 0.9672 - val_loss: 0.1060\n",
      "Epoch 287/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9717 - loss: 0.0879\n",
      "Epoch 287: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9717 - loss: 0.0879 - val_accuracy: 0.9677 - val_loss: 0.1105\n",
      "Epoch 288/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9725 - loss: 0.0841\n",
      "Epoch 288: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9725 - loss: 0.0841 - val_accuracy: 0.9667 - val_loss: 0.1054\n",
      "Epoch 289/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9740 - loss: 0.0829\n",
      "Epoch 289: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9740 - loss: 0.0830 - val_accuracy: 0.9658 - val_loss: 0.1097\n",
      "Epoch 290/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9665 - loss: 0.0964\n",
      "Epoch 290: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9665 - loss: 0.0964 - val_accuracy: 0.9649 - val_loss: 0.1215\n",
      "Epoch 291/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9704 - loss: 0.0856\n",
      "Epoch 291: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9704 - loss: 0.0857 - val_accuracy: 0.9689 - val_loss: 0.1061\n",
      "Epoch 292/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9682 - loss: 0.0897\n",
      "Epoch 292: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9682 - loss: 0.0897 - val_accuracy: 0.9656 - val_loss: 0.1112\n",
      "Epoch 293/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9702 - loss: 0.0936\n",
      "Epoch 293: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9702 - loss: 0.0935 - val_accuracy: 0.9658 - val_loss: 0.1124\n",
      "Epoch 294/300\n",
      "\u001b[1m396/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9699 - loss: 0.0908\n",
      "Epoch 294: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9699 - loss: 0.0908 - val_accuracy: 0.9653 - val_loss: 0.1077\n",
      "Epoch 295/300\n",
      "\u001b[1m395/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9720 - loss: 0.0925\n",
      "Epoch 295: val_accuracy did not improve from 0.96887\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9720 - loss: 0.0926 - val_accuracy: 0.9665 - val_loss: 0.1050\n",
      "Epoch 296/300\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9692 - loss: 0.0901\n",
      "Epoch 296: val_accuracy improved from 0.96887 to 0.96910, saving model to best_model.keras\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9692 - loss: 0.0901 - val_accuracy: 0.9691 - val_loss: 0.1036\n",
      "Epoch 297/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9700 - loss: 0.0879\n",
      "Epoch 297: val_accuracy did not improve from 0.96910\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.9700 - loss: 0.0879 - val_accuracy: 0.9672 - val_loss: 0.1049\n",
      "Epoch 298/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9718 - loss: 0.0914\n",
      "Epoch 298: val_accuracy did not improve from 0.96910\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9718 - loss: 0.0914 - val_accuracy: 0.9649 - val_loss: 0.1130\n",
      "Epoch 299/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9725 - loss: 0.0789\n",
      "Epoch 299: val_accuracy did not improve from 0.96910\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9725 - loss: 0.0789 - val_accuracy: 0.9660 - val_loss: 0.1197\n",
      "Epoch 300/300\n",
      "\u001b[1m397/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9743 - loss: 0.0808\n",
      "Epoch 300: val_accuracy did not improve from 0.96910\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9742 - loss: 0.0809 - val_accuracy: 0.9660 - val_loss: 0.1152\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the filepath to save the best model\n",
    "filepath = \"best_model.keras\"\n",
    "\n",
    "# Define the ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# Compile your model before training\n",
    "# model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train your model with the ModelCheckpoint callback\n",
    "history = model.fit(X_train, y_train_encoded, epochs=300, validation_data=(X_val, y_val_encoded), callbacks=[checkpoint])\n",
    "\n",
    "# After training, you can load the best model using:\n",
    "# best_model = keras.models.load_model(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9616 - loss: 0.1388\n",
      "Test Loss: 0.13272742927074432\n",
      "Test Accuracy: 0.963443398475647\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"Accuracy of our model on test data : \" , model.evaluate(X_test,y_test_encoded)[1]*100 , \"%\")\n",
    "import matplotlib.pyplot as plt\n",
    "epochs = [i for i in range(300)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "test_acc = history.history['val_accuracy']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,6)\n",
    "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "ax[0].plot(epochs , test_loss , label = 'Validation Loss')\n",
    "ax[0].set_title('Training & Validation Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "ax[1].plot(epochs , test_acc , label = 'Validation Accuracy')\n",
    "ax[1].set_title('Training & Validation Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "y_predict = np.argmax(model.predict(X_test), axis=-1)\n",
    "accuracy = accuracy_score(y_test_encoded, y_predict)\n",
    "precision = precision_score(y_test_encoded, y_predict, average='weighted')\n",
    "recall = recall_score(y_test_encoded, y_predict, average='weighted')\n",
    "f1 = f1_score(y_test_encoded, y_predict, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data accuracy:  96.34433962264151\n",
      "Test Data accuracy:  96.34433962264151\n",
      "Precision:  0.9635232163609705\n",
      "Recall:  0.9634433962264151\n",
      "F1 Score:  0.9634304711897994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "y_predict = np.argmax(model.predict(X_test), axis=-1)\n",
    "accuracy = accuracy_score(y_test_encoded, y_predict)\n",
    "precision = precision_score(y_test_encoded, y_predict, average='weighted')\n",
    "recall = recall_score(y_test_encoded, y_predict, average='weighted')\n",
    "f1 = f1_score(y_test_encoded, y_predict, average='weighted')\n",
    "print('Test Data accuracy: ',accuracy_score(y_test_encoded, y_predict)*100)\n",
    "print('Test Data accuracy: ', accuracy * 100)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1 Score: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       600\n",
      "           1       0.98      0.97      0.97       624\n",
      "           2       0.96      0.98      0.97       603\n",
      "           3       0.97      0.96      0.96       592\n",
      "           4       0.95      0.96      0.96       668\n",
      "           5       0.96      0.94      0.95       586\n",
      "           6       0.96      0.98      0.97       567\n",
      "\n",
      "    accuracy                           0.96      4240\n",
      "   macro avg       0.96      0.96      0.96      4240\n",
      "weighted avg       0.96      0.96      0.96      4240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_encoded, y_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
